[{"categories":["Post","Cryptography","Security","Privacy","Cryptocurrency"],"content":"This post is a gentle introduction to shielded transactions, as used in private payment systems such as ZCash, Penumbra or on top of any Bitcoin-shaped (i.e. UTXO-based) protocol. At the end of this post, this figure will mean something to you:\nBut there’s a lot there, so let’s break it down.\nMotivation: Bitcoin is not private As you probably know, Bitcoin is peer-to-peer electronic cash. It’s non-custodial: I don’t need to trust a third party to hold my keys or my money. It’s decentralized, in that there is no single point of failure: I don’t need to trust a small subset of nodes or a single node is behaving honestly. These are great properties, and we want to preserve them.\nHowever, Bitcoin achieves these properties at the cost of privacy. Every Bitcoin transaction is recorded on the blockchain, and is visible to everyone. It’s “twitter for your bank account”.\nWhile Bitcoin is pseudonymous - using public key hashes instead of real names - this is not enough to provide privacy. This pseudonymity is easily broken: firms like Chainalysis and others make a business out of de-anonymizing bitcoin users, connecting pseudonymous bitcoin addresses to real-world identities.\nThese firms make use of the fact that the Bitcoin blockchain provides:\nthe clear plaintext value of the transaction, e.g. 1 BTC the pseudonymous recipient identity, i.e. the recipient is identified by a hash of their ECDSA public key A passive observer can use this information about the transaction graph of money flows to deanonymize users.\nIf you believe that privacy in an open society requires anonymous transaction systems, then you’ll be happy to hear that there are solutions!\nWhat do we want? Privacy with integrity We want a cryptocurrency that is private, so ideally we want one in which a passive observer cannot learn anything about values, sender or recipient identities.\nBut, we also need to enforce all the integrity properties that Bitcoin provides. We need to ensure properties like:\nYou cannot spend coins that don’t exist You cannot double-spend coins You cannot spend other people’s coins You cannot create or destroy value What’s the problem? Why is this so hard? Let’s take a step back and recall how Bitcoin does any of this.\nRecap: Bitcoin transaction structure How does a Bitcoin transaction work?\nAbove is a simplified diagram of a Bitcoin transaction - at least the parts we need to care about.\nWe have a list of $i$ inputs on the left, and $j$ outputs on the right.\nOutputs Let’s start at the right, with the outputs.\nEach output has:\na value $v$ and a recipient, identified by a public key $r$. Technically the recipient is a specification in Bitcoin script, but we’ll ignore that as it’s not important for our purposes. All we need to know, is that in the simple payments case, the specification is just saying: “hey, here’s the public key of the recipient, and in the future, they need to present a valid signature $\\sigma$ that is verified using this public key $r$ in order to spend this output”.\nInputs Now the inputs.\nThe inputs each reference a previously Unspent Transaction Output (UTXO). Each UTXO has a certain value associated with it. Each inputs unlocks the value in that UTXO by presenting a signature $\\sigma$ that can be verified using the public key of the recipient $r$ in the UTXO.\nIntegrity Now, let’s think back to our desired system properties. We already know Bitcoin doesn’t provide privacy, but what about the integrity properties we enumerated above? How does Bitcoin achieve these integrity properties?\n1. You cannot spend coins that don’t exist Every spend (input) references a UTXO. Nodes scan and check that the referenced UTXO exists, else they reject the transaction.\n2. You cannot double-spend coins When we reference a UTXO in an input, nodes will also scan and check that the referenced UTXO has not been spent before. Each UTXO can only be spent once. If a UTXO has been spent, the node will reject the transaction.\n3. You cannot spend other people’s coins When we spend a coin, i.e. reference a UTXO in an input, we include a digital signature $\\sigma$, signed with our private key. This signature is a bit of data that lets anyone in possession of our public key verify that this is my coin. Nodes are going to reject any attempt to spend a coin that you don’t have authorization to spend.\n4. You cannot create or destroy value As we’ve established, the values in each transaction are in plaintext, so we can simply calculate that the sum of the inputs equals the sum of the outputs (modulo fees, which we’ll ignore here). This ensures that no value is created out of thin air.\nProperties of a private cryptocurrency Now, let’s think about what needs to change to achieve privacy. We’re going to continue to use a Bitcoin-shaped protocol, and we’ll use the term “coins” and “UTXOs” interchangeably.\nPrivacy We want to ensure that a passive observer cannot learn anything about the value, sender or recipient identities.\nWe’ll do that by simply encrypting all those fields. There are some details to work out, but we’ll stick with the naive idea of encrypting all the data in the transaction.\nIntegrity How does this impact the integrity properties we enumerated above?\n1. You cannot spend coins that don’t exist Here we have a problem: we established that an observer such as a node won’t have access to the transaction graph of money flows, so we can’t have nodes check references to UTXOs in the transaction graph. We need another way to check that the referenced UTXO exists.\n2. You cannot double-spend coins This is the same problem as for spending coins that don’t exist: we can’t check references to UTXOs in the transaction graph.\n3. You cannot spend other people’s coins We definitely need a way to authorize the spending of coins, so we still need to use digital signatures. But we have a problem, because naively using digital signatures like Bitcoin introduces a privacy issue.\nIf I want to find what my friend Jim is doing on the blockchain, well, I have his public key, because it’s public. So I can trial verify each signature on each spend of a UTXO, and if the signature verifies, I’ve identified Jim’s spends. That violates the privacy property. We’re not supposed to be able to learn anything about Jim’s behavior.\nSo, we need to do something different.\n4. You cannot create or destroy value We can’t check the value balance in a transaction by naively summing up the values of the inputs and outputs, because the values are encrypted.\nBuilding a private and decentralized UTXO-based protocol We need to find a way to encrypt the values, sender and recipient identities, while still being able to do the integrity checks we described above.\nThis is a problem that researchers have been working on for over a decade. The first paper tackling this problem, Zerocoin, succeeded in creating a decentralized payments system unlinking transaction origin from sender. However, it does this with fixed size coins.\nZerocash, a followup paper, improved on this scheme. It introduced a decentralized anonymous payment scheme where sender, recipient and amount are hidden - and their scheme also allows for variable amounts. This ultimately evolved into Zcash, and we’re going to roughly describe the ZCash sapling protocol design in the rest of this post.\nWhat we’re going to do is walk through each of the informal privacy and integrity properties we’ve been describing, and see how shielded transactions like those in Zcash (or Zcash-derived protocols) achieve them.\nPrivacy We’re going to carry value in notes. A plaintext note consists of at least:\na value $v$, a recipient $r$ (i.e. an address), a bit of randomness we’ll call a “blinding factor” that we’ll need later. Notes are going to be encrypted and then posted on chain as part of a transaction.\nEach note is going to be used only once. It gets minted, then it is spent, and then it is no longer valid. When you spend a note, you release the value of that note into the transaction which can then be used to mint other notes. This is very similar to the UTXO model of Bitcoin: each UTXO can only be spent once, and when it is spent, the value is released into the transaction.\nKey Hierarchy One interesting feature of ZCash-style cryptocurrencies is that there is a separation of capabilities in the key hierarchy.\nIn general, we have a spending key $sk$, that lets us spend coins, and a full viewing key $fvk$, that lets us view our part of the transaction graph.\nWhen we sync the blockchain, we’ll need to trial decrypt each note ciphertext with our viewing key to see if it decrypts to a valid note. If it does, it’s a note intended for us, and one that we can spend using our spending key.\nShielded Transaction Structure Transactions consist of multiple actions.\nThere are two types of actions we’re going to discuss in this post: inputs/spends and outputs. We’ll call a Bitcoin-style input a spend, since that’s a better name anyway.\nSo far, our picture of a shielded transaction looks like this:\nWe also know that our outputs are creating a new note, encrypting it to the recipient, and posting it to the chain as a note ciphertext $(\\textbf{C}_j)$. Let’s add that to our picture:\nGreat. Let’s see what other pieces we need to add to our picture.\nIntegrity 1. You cannot spend coins that don’t exist To validate transactions, we somehow need nodes to keep track of two data structures:\nAll notes that exist in the system All notes that have been spent in the system We need to do this in such a way that an observer (including the node) cannot map items in data structure 1 — all notes in the system — to items in data structure 2 — all notes that have been spent in the system. If nodes could do that, we’ve got the transaction graph of money flows again, and avoiding that was the whole point of this exercise!\nWe are going to instead derive a quantity from each note that:\nBinds us to the value $v$ and recipient $r$ Hides the value $v$ and recipient $r$ This is exactly what we get from the binding and hiding properties of a cryptographic commitment scheme. This is also why we needed the blinding factor in our notes: it is used for generating the note commitment.\nNodes are going to store in a special data structure a cryptographic commitment to every single note that has ever existed in the system. We’ll discuss later what this data structure is.\nThus far, we just had our outputs each with a note ciphertext $(\\textbf{C}_j)$:\nWe’ll also add a cryptographic commitment to each output $(\\textbf{cm}_j)$. Now our picture of a shielded transaction looks like this:\nThe commitment is binding us to the value and the recipient in the note, such that the recipient cannot later claim “oh hey my 1 BTC note? it was actually 100 BTC”.\nIf a node validates a transaction, they then for each output are going to add the note commitment to a data structure that keeps track of every single note commitment in the system.\nAt this point, we’ll need to start using Zero Knowledge Proofs (ZKP). In brief, a ZKP demonstrates a particular statement is true, without revealing any information about the statement other than its veracity. In our setting, the party generating the proofs, the prover, will be the person preparing the transaction, and the person checking the proofs are valid, the verifier, will be the nodes validating the transaction. We’ll be using ZKPs in various places in the protocol to make statements about private bits of data, that we’ll call the “witness”.\nFor example, for each one of our outputs, we’ll need a little ZKP. This output proof $\\pi_j$ is going to demonstrate that the note commitment is well-formed. We’ll witness the value $v_j$ and the recipient $r_j$ in the note, and the node will verify the proof to check that the public note commitment is derived correctly.\nLet’s add that output proof, $\\pi_j$, to our picture:\nOutput Circuit One of the tricky parts about ZKPs is that proof systems require the computation or statements to be represented in a way that the proof system can understand. Typically, this is done by representing the computation as an arithmetic circuit, where the gates in the circuit are arithmetic operations such as addition. We’ll need to write down as a circuit the logic that we want to prove.\nHere’s what our circuit will need to do at a high level (so far) for our output actions:\nNodes keep track of all notes in the system Let’s go back to how nodes are going to go about storing the two data structures: one of all notes in the system, and one of all notes that have been spent.\nWell, we could just store a big list of all the notes in the system. But when we spend, we need a way to demonstrate that our note commitment is in this set. However, this is a pretty inefficient proof of size O(N) where N is the number of notes in the system. Maybe we can do better?\nNodes maintain an incremental Merkle tree of all note commitments Instead, we’re going to keep track of all the note commitments in the system using a Merkle tree.\nA Merkle tree is simply a tree in which each internal node is the hash of its children.\nWe’re also going to have it be append-only, and we’re going to incrementally update the tree when we add notes by filling the next leaf node. Crucially, we will never delete from the tree, since that would leak information about activity on the network.\nWe’ll have each leaf node contain a note commitment.\nBy using a Merkle tree, we can compress a large amount of data into a small, fixed-size value: the hash of the root of the tree, or the anchor.\nMerkle proofs let us demonstrate our note is in the system This lets us do proofs in a succinct way. We can prove set membership via a Merkle proof. We can construct a Merkle authentication path that consists of the siblings of each node on the path from our note commitment to the root of the tree. If the note is truly in the tree, when we hash together the note commitment with its siblings all the way up to the tree root, we should arrive at the public anchor.\nThe depth of the tree is fixed and is a constant set by the network. So the proofs are going to be the fixed-size (depth of the tree multiplied by the number of siblings), and they’re going to be small.\nWe do these Merkle proofs in a ZKP But wait, we can’t provide our Merkle authentication path to the node/verifier, because then the verifier can learn something about our note commitment.\nSo, to make these proofs private, we need to do the Merkle proof inside a ZKP.\nGreat. That will prove to the verifier that the note commitment is in the tree meaning that it exists in the system. And we can add a little more to our picture of a shielded transaction: the spend ZKP $\\pi_i$.\n2. You cannot double-spend coins. We’ve described our first node data structure, tracking all notes that ever existed in the system. What about the second data structure, the set of all notes that have been spent in the system?\nFor each note, we’re going to define a way of deriving a special value, called a nullifier $nf$. It’s effectively a serial number for the note. Critically, there can only be one valid nullifier per note. If you find a trick that lets you derive another nullifier that will be considered valid for that note, that will constitute a way to double spend.\nNodes are going to store in a second data structure the set of all nullifiers that have ever been revealed:\nWhen we do a spend, we are going to reveal the nullifier associated with the note. Once revealed, we cannot spend the same note again.\nNodes will check as part of transaction verification that the nullifier in a spend is not in the nullifier set. If it is, the transaction is rejected.\nSo we’re going to add the nullifier to our picture. For each spend, we reveal the nullifier $nf_i$.\nSpend Circuit We’re also going to add to the spend ZKP a check that the nullifier has been derived correctly from the note being spent.\nHere’s what our circuit now looks like for our spend actions:\nNode state management We now have the two data structures that nodes will need to maintain:\nThe incremental Merkle tree of all note commitments in the system. The nullifier set that corresponds to all spent notes in the system. 3. You cannot spend other people’s coins Recall from earlier in the post that we can’t naively use regular signature schemes in privacy-preserving protocols because they leak information about the signer’s identity. An observer can trivially link spends by doing trial verification using public keys of their targets.\nInstead we use a re-randomizable signature scheme.\nWe derive a one-time use (“randomized”) key $rk$ from our real key $ak$, and use that:\n$rk = ak + [\\alpha]B$\nWe’ll provide the one-time use verification key $rk_i$ on each spend:\nSpend Circuit We also need to demonstrate in our ZKP that the randomized key (public on the transaction) is a correct randomization given the witnessed real key $ak$ and randomizer $\\alpha$.\nHere’s what our circuit looks like so far for our spend actions:\n4. You cannot create or destroy value Finally, we need to ensure that the sum of the values in the inputs equals the sum of the values in the outputs.\nHere we need to take a little detour into the properties of Pedersen commitments.\nPedersen commitments are additively homomorphic.\nA homomorphism is just a function between two algebraic structures that preserves their operations, meaning it keeps the structure’s properties intact when applied. An additive homomorphism is a type of homomorphism that specifically preserves addition. Let’s see an example to make clear how this works and how it helps us:\nWe have a Pedersen commitment scheme which generates a commitment $cm$ using an algorithm $\\texttt{Commit}$ that takes a value $m$ and some randomness $r$ as follows:\n$cm = \\texttt{Commit}(m, r) = [m]G + [r]H$\nG and H are going to be constants that we pick as part of the protocol.\nGiven that definition, let’s now assume we have two commitments $cm_1$ and $cm_2$. Using the definition above we have:\n$cm_1 = \\texttt{Commit}(m_1, \\texttt{randomness}_1) = [m_1]G + [\\texttt{randomness}_1]H$\n$cm_2 = \\texttt{Commit}(m_2, \\texttt{randomness}_2) = [m_2]G + [\\texttt{randomness}_2]H$\nIf we add our two commitments together, $cm_1 + cm_2$, we get:\n$cm_1 + cm_2$\n$= [m_1]G + [\\texttt{randomness}_1]H + [m_2]G + [\\texttt{randomness}_2]H$\nRearranging, we get:\n$cm_1 + cm_2 = [m_1 + m_2]G + [\\texttt{randomness}_1 + \\texttt{randomness}_2]H$\nAnd that is equivalent to:\n$cm_1 + cm_2= \\texttt{Commit}(m_1 + m_2, \\texttt{randomness}_1 + \\texttt{randomness}_2)$\nWhat does this mean? It means that if we have two commitments, we can add them together to get a new commitment to the sum of the values being committed to. At no point in this did we learn anything about the individual values.\nValue commitments We’re going to add a value commitment which we’ll call $cv$ to every single action in the transaction.\nIt’ll be derived from the relevant note’s value $v$ and a bit of randomness $\\texttt{randomness}$:\n$cv = [v]G + [\\texttt{randomness}]H$\nWe’ll adopt a convention where $v$ is the positive when we’re doing a spend (releasing value into the transaction), and $v$ is negative when we’re doing an output (consuming value from the transaction).\nLet’s add those value commitments $cv_i$ and $cv_j$ to our picture of a shielded transaction:\nEvery single action has a value commitment. And we can sum all the value commitments. If the transactino balances, then the values should cancel out: the positive value should balance with the negative value. This is done by checking that the sum of the value commitments ($\\sum_{i}\\sum_{j}(cv_{i,j})$) is a commitment to zero.\nWe’ll also need to include in the transaction the sum of the blinding factors, such that we can check:\n$\\sum_{i}\\sum_{j}(cv_{i,j}) = \\texttt{Commit}(0, \\sum_{i}\\sum_{j}(\\texttt{randomness}_{i,j}))$\nAnd, we’ll also need to include in each circuit a check that the value commitments are well-formed from the relevant note.\nSummary: How Shielded Transactions Work Remember that figure from the beginning? Now we understand each component of it:\nIn summary:\nAll notes are encrypted on the blockchain: the chain never sees recipient, sender, or value. The note commitment tree is an incremental Merkle tree that is an append-only data store of all notes in the system. Spends of a note must demonstrate in a zero knowledge proof that the note commitment is in the commitment tree. Notes are nullified/deleted by revealing a nullifier (once, constituting double spend protection) that goes into the nullifier set. Observers cannot link nullifier to notes that were invalidated. Spends also must demonstrate control of the note via a randomized signature. Value conservation is provided through the additively homomorphic property of value commitments. As you might imagine, there’s a lot of detail that I’ve glossed over here. If you’re interested in learning more, I recommend checking out the ZCash protocol specification.\n","description":"","tags":["Bitcoin","privacy","cryptography"],"title":"A gentle introduction to shielded transactions","uri":"/post/utxo_privacy/"},{"categories":["cryptography","zero-knowledge proofs"],"content":"Shielded blockchains like Penumbra provide privacy through the use of zero-knowledge proofs (ZKPs): actions that change the public chain state can be verified without providing the underlying private data.\nNote: I wrote this blog for the Penumbra Labs site. Below is an abbreviated version. You can see the original post here.\nTransparent “Proofs” Our plan for implementing Penumbra has been to use an approach which allows quick iterations on the design of the system without spending significant effort each iteration to update zero-knowledge circuits. To ensure that the design of the system was kept compatible with zero-knowledge proofs, each action had a “transparent proof”, for example for spending notes:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 /// Transparent proof for spending existing notes. /// /// This structure keeps track of the auxiliary (private) inputs. #[derive(Clone, Debug)] pub struct SpendProof { // Inclusion proof for the note commitment. pub state_commitment_proof: tct::Proof, // The note being spent. pub note: Note, // The blinding factor used for generating the value commitment. pub v_blinding: Fr, // The randomizer used for generating the randomized spend auth key. pub spend_auth_randomizer: Fr, // The spend authorization key. pub ak: VerificationKey\u003cSpendAuth\u003e, // The nullifier deriving key. pub nk: keys::NullifierKey, } impl SpendProof { /// Called to verify the proof using the provided public inputs. /// /// The public inputs are: /// * the merkle root of the state commitment tree, /// * value commitment of the note to be spent, /// * nullifier of the note to be spent, /// * the randomized verification spend key, pub fn verify( \u0026self, anchor: tct::Root, balance_commitment: balance::Commitment, nullifier: Nullifier, rk: VerificationKey\u003cSpendAuth\u003e, ) -\u003e anyhow::Result\u003c()\u003e { // ... } This SpendProof struct trivially stored the witnesses in cleartext. The prover created this struct, and sent it to the node, where a verification method was called using the public inputs provided in the transaction. The verification method did all the integrity checks the real proof would: verifying the Merkle path, checking the prover had an opening of the public commitment, and so on. This did not provide privacy, but it let us rapidly prototype the system while refining the protocol design, with assurance that when our requirements became stable, we could fill in the proofs.\nZero-Knowledge As we approach mainnet and the system functionality becomes stable, we began migrating from transparent proofs to zero-knowledge proofs starting with testnet 46, codenamed Lysithea, released on February 27th, 2023. Now that Penumbra’s multi-asset shielded pool is stable, that release migrated outputs (actions that create new notes) and spends (actions that consume existing notes) to use zero-knowledge proofs. Interaction with Penumbra’s DEX, governance, and staking systems will follow.\nOne of Penumbra’s design goals is to create a usable privacy system. That means fast proving times: at mainnet we’re aiming for proving times below one second on end-user devices. We can do this by performing the proving for all actions concurrently and by using Groth16. For Penumbra’s initial ZKPs, we use the pairing-friendly BLS12-377 proving curve and the Arkworks implementation of the Groth16 proving system. It has excellent out-of-the-box performance even before optimization: on an M1 macbook, transactions with three actions (one spend, two outputs) typically take under 1.3s to generate. We also get the benefits of very small proof size and using a mature system that has been in production for years.\nA disadvantage of Groth16 is that it requires a circuit-specific setup, meaning each time we change our proof statements, we need to re-run a decentralized setup procedure to generate new parameters for the prover and verifier. The requirement for this process is there is at least one honest participant in the setup, thus motivating a large setup process involving many participants. Stay tuned for more details on the setup procedure and how you can participate!\nOur Spend proof from above now looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 pub struct SpendProof(Proof\u003cBls12_377\u003e); impl SpendProof { pub fn prove\u003cR: CryptoRng + Rng\u003e( rng: \u0026mut R, pk: \u0026ProvingKey\u003cBls12_377\u003e, state_commitment_proof: tct::Proof, note: Note, v_blinding: Fr, spend_auth_randomizer: Fr, ak: VerificationKey\u003cSpendAuth\u003e, nk: NullifierKey, anchor: tct::Root, balance_commitment: balance::Commitment, nullifier: Nullifier, rk: VerificationKey\u003cSpendAuth\u003e, ) -\u003e anyhow::Result\u003cSelf\u003e { let circuit = SpendCircuit { state_commitment_proof, note, v_blinding, spend_auth_randomizer, ak, nk, anchor, balance_commitment, nullifier, rk, }; let proof = Groth16::prove(pk, circuit, rng).map_err(|err| anyhow::anyhow!(err))?; Ok(Self(proof)) } /// Called to verify the proof using the provided public inputs. pub fn verify( \u0026self, vk: \u0026PreparedVerifyingKey\u003cBls12_377\u003e, anchor: tct::Root, balance_commitment: balance::Commitment, nullifier: Nullifier, rk: VerificationKey\u003cSpendAuth\u003e, ) -\u003e anyhow::Result\u003c()\u003e { // ... } } Our ZK proofs are now just three group elements in size. The prover uses provided proving parameters (type ProvingKey\u003cBls12_377\u003e), which we distribute via a penumbra-proof-params crate, to create the proof using their private witnesses and public inputs. The verifier uses the corresponding verifying key (type PreparedVerifyingKey\u003cBls12_377\u003e) in order to verify the proofs on the node using the public inputs provided in the transaction.\nCircuit Programming To generate the circuit for each action, we first need to represent the statements we want to prove - for example that the prover knows an opening of a specific public commitment - in a way that our proving system can understand. For Groth16 proofs, this means representing all statements to be proved in-circuit as a rank-1 constraint system (R1CS). We need to be able to write down elliptic curve operations, hash function evaluations and so on, as a number of constraint equations that are simple linear combinations of field element variables.\nSeveral of our dependencies now have this R1CS functionality: decaf377, poseidon377, and penumbra-tct all have an optional r1cs feature, while penumbra-crypto has R1CS functionality inline next to each type that needs to be represented in-circuit. This lets us do elliptic curve operations, SNARK-friendly hashing, and all other operations in-circuit. For example, here is the type that represents in-circuit which path a node in our tiered commitment tree can take:\n1 2 3 4 5 6 7 8 9 10 11 12 13 /// Represents the different paths a quadtree node can take. /// /// A bundle of boolean R1CS constraints representing the path. pub struct WhichWayVar { /// The node is the leftmost (0th) child. pub is_leftmost: Boolean\u003cFq\u003e, /// The node is the left (1st) child. pub is_left: Boolean\u003cFq\u003e, /// The node is the right (2nd) child. pub is_right: Boolean\u003cFq\u003e, /// The node is the rightmost (3rd) child. pub is_rightmost: Boolean\u003cFq\u003e, } We can see in-circuit the path of the node at a given height is represented by four boolean constraints. The Fq type here just represents the type of the field elements used by the proving system.\nThese R1CS types are used during constraint synthesis. We write Rust code to define types that define bundles of constraints. We then use those types along with the Arkworks ConstraintSystem\u003cFq\u003e, which internally keeps track of all the R1CS constraints we build up by:\nallocating witness or input variables, defining constants, or performing an operation on defined variables or constants. An upstream Arkworks trait called ConstraintSynthesizer is implemented for each of our circuit/actions. Here’s part of the implementation for our Spend circuit:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 impl ConstraintSynthesizer\u003cFq\u003e for SpendCircuit { fn generate_constraints(self, cs: ConstraintSystemRef\u003cFq\u003e) -\u003e ark_relations::r1cs::Result\u003c()\u003e { // Witnesses let note_var = note::NoteVar::new_witness(cs.clone(), || Ok(self.note.clone()))?; … let v_blinding_vars = UInt8::new_witness_vec(cs.clone(), self.v_blinding.to_bytes())?; // Public inputs let claimed_balance_commitment_var = BalanceCommitmentVar::new_input(cs.clone(), || Ok(self.balance_commitment))?; … // Check integrity of balance commitment. let balance_commitment = note_var.value().commit(v_blinding_vars)?; balance_commitment .enforce_equal(\u0026claimed_balance_commitment_var)?; // ... Ok(()) } } In our abridged and slightly simplified constraint synthesis example here, we can see that we first witness a NoteVar, providing a reference to the underlying constraint system. This allocates a variable in-circuit, adding constraints as we go.\nNext, we define a public balance commitment, which represents a commitment to the value balance of this action. The public balance commitment we call claimed_balance_commitment_var as it represents the public value of the balance commitment: the verifier needs to certify that the balance commitment was computed correctly, using private variables it does not have access to on the NoteVar. The prover adds constraints to demonstrate that by calling commit on the value of the NoteVar, and adding constraints that the output of the commit method must be equal to the corresponding public input.\nIn a similar fashion, we can build up all constraints in an ergonomic manner by writing regular Rust code.\n","description":"","tags":["cryptography"],"title":"Bringing Zero-Knowledge Proofs to Penumbra","uri":"/post/zkproofs-intro/"},{"categories":["cryptography","hash functions"],"content":"Penumbra is building a shielded, cross-chain network for private transactions, staking, swaps, and marketmaking. We use zero-knowledge proofs (ZKP) to provide privacy. One crucial component for efficient ZKPs is a hash function that is efficient in the circuit context. In this blog post we provide an overview of the hash function Poseidon, introduce Poseidon377 - our instantiation of the Poseidon ZKP-friendly hash for the Penumbra system - and two of its dependent crates which are generic over Arkworks prime fields: poseidon-permutation, which provides an independent implementation of the Poseidon permutation, and poseidon-paramgen, which provides a fully audited (by NCC Group in Summer 2022) and independent implementation of Poseidon parameter generation.\nNote: I wrote this blog for the Penumbra Labs site. You can see the original post here.\nSNARK-friendly hashes Penumbra ensures that state transitions are valid while protecting privacy using ZKPs, specifically ZK-SNARKs (Zero-Knowledge Succinct Non-Interactive ARgument of Knowledge). One of the challenges building ZK-SNARKs is the inefficiency of traditional hash algorithms like the SHA2 family, which are designed to use many bitwise operations. This makes them fast when executing on a CPU, but prohibitively slow in the context of a SNARK circuit, where bitwise operations are very expensive, as each bit must be represented by a single constraint.\nFor the Penumbra protocol, we need to hash in-circuit in many places. For example, we want to prove that:\nA balance commitment, which is used to check the value balance of a Penumbra transaction, was computed correctly, A note commitment was computed correctly, which are used to track all existing notes in Penumbra’s tiered commitment tree, A nullifier was derived correctly, which are used to prevent double-spends, A note was previously included in Penumbra’s (Merkle) tiered commitment tree, which ensures only existing notes can be spent. Poseidon, first introduced by Lorenzo Grassi, Dmitry Khovratovich, Christian Rechberger, Arnab Roy, and Markus Schofnegger, is a cryptographic hash function that is efficient in the context of a SNARK circuit using R1CS arithmetization, due to the fact it operates natively over field elements.\nHow Poseidon works Poseidon uses a sponge construction, wherein the internal state is populated from some input elements, often described as the elements being “absorbed” by the sponge. A fixed-length internal permutation is applied to that internal state. Output elements are then extracted — or “squeezed”— from the internal state.\nThe inner Poseidon permutation is a substitution-permutation network, similar to AES, except instead of operating over an internal state in the form of a matrix of bytes, it operates over an internal state of a vector of field elements in some specified prime field. The permutation consists of rounds, where each round has the following steps:\nAddRoundConstants: where constants are added to the internal state, SubWords: where the S-box S(x)=x^α is applied to the internal state, MixLayer: where a matrix is multiplied with the internal state. The total number of rounds we denote by R. There are two types of round in the Poseidon construction, partial and full. We denote the number of partial and full rounds by R_P​ and R_F​ respectively.\nPartial vs Full Rounds In a full round in the SubWords layer, the S-box is applied to each element of the internal state, as shown in the diagram below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ┌───────────────────────────────────────────────────────────┐ │ │ │ AddRoundConstants │ │ │ └────────────┬──────────┬──────────┬──────────┬─────────────┘ │ │ │ │ ┌─▼─┐ ┌─▼─┐ ┌─▼─┐ ┌─▼─┐ │ S │ │ S │ │ S │ │ S │ └─┬─┘ └─┬─┘ └─┬─┘ └─┬─┘ │ │ │ │ ┌────────────▼──────────▼──────────▼──────────▼─────────────┐ │ │ │ MixLayer │ │ │ └────────────┬──────────┬──────────┬──────────┬─────────────┘ │ │ │ │ ▼ ▼ ▼ ▼ In a partial round in the SubWords layer, we apply the S-box only to one element of the internal state, as shown in the diagram below:\n│ │ │ │ │ │ │ │ ┌────────────▼──────────▼──────────▼──────────▼─────────────┐ │ │ │ AddRoundConstants │ │ │ └────────────┬──────────────────────────────────────────────┘ │ ┌─▼─┐ │ S │ └─┬─┘ │ ┌────────────▼──────────────────────────────────────────────┐ │ │ │ MixLayer │ │ │ └────────────┬──────────┬──────────┬──────────┬─────────────┘ │ │ │ │ ▼ ▼ ▼ ▼ We apply half the full rounds (R_f​=R_F​/2) first, then we apply the R_P​ partial rounds, then the rest of the R_f​ full rounds. This approach - wherein the middle layer consists of partial rounds, and is sandwiched by full rounds - is called the HADES design strategy in the literature.\nPoseidon377, our poseidon implementation poseidon377 is our instantiation of Poseidon over the decaf377 group. It is a thin wrapper exposing only the fixed-width hash functions we need for the Penumbra protocol. The bulk of the interesting logic is in two dependent general-purpose Rust crates: poseidon-permutation, and poseidon-paramgen.\nCrate poseidon-permutation is an optimized implementation of the Poseidon permutation. The optimized Poseidon permutation uses sparse matrices in the partial layers to reduce the number of multiplications that need to be performed, resulting in significant speedups: the number of multiplications in the partial layers goes from quadratic in the size of the internal state to linear. This is especially important for instantiations of Poseidon with a large width or a large number of partial rounds.\nParameter Generation For an instantiation of Poseidon, the parameters we need to generate consist of:\nS-box: choosing the exponent α for the S-box in SubWords step where S(x)=x^α, Round numbers: the numbers of partial and full rounds, i.e. R_P and R_F, Round constants: the constants to be added in the AddRoundConstants step, MDS Matrix: generating a Maximum Distance Separable (MDS) matrix to use in the MixLayer, where we multiply this matrix by the internal state. The problem of Poseidon parameter generation is to pick secure choices for the parameters to use in the permutation given the field, desired security level M in bits, as well as the width t of the hash function one wants to instantiate (i.e. 1:1 hash, 2:1 hash, etc.).\nOne of the challenges with the Poseidon hash function is there is not a universal set of parameters. A useful analogy is if traditional CPUs had different types of bits, let’s say Blue bits and Red bits, and you had to customize SHA2 for whether you want to run it on a system with Red bits or Blue bits. While there may be optimizations that are platform-specific in traditional hash functions, the hash function itself is the same. This is not the case with Poseidon, where the parameters that define the function depend on the underlying finite field.\nThe Poseidon paper provides sample implementations of both the Poseidon permutation as well as parameter generation. There is a Python script called calc_round_numbers.py which provides secure choices for the round numbers given the security level M, the width of the hash function t, as well as the choice of S-box to use. There is also a Sage script, which generates the round numbers, constants, and Maximum Distance Separable (MDS) matrix, given the security level M, the width of the hash function t, as well as the choice of S-box.\nSince the publication of the Poseidon paper, others have edited these scripts for their own projects, resulting in a number of implementations being in use derived from these initial scripts. Of particular note is Filecoin, who have an implementation of Poseidon and parameter generation in Rust. As an independent check, we elected to implement Poseidon parameter generation in Rust from the original paper using the Arkworks family of crates which we use throughout Penumbra’s cryptography libraries.\nOur poseidon-paramgen crate is the result: an independent implementation of Poseidon parameter generation that generates parameters based on the desired security level in bits, the desired width of the hash function, and the prime modulus of the field. We checked each step as described in the paper, and additionally automated the S-box parameter selection step. It also includes logic to generate code containing the parameters in Rust at build time, which we use for poseidon377. We describe in further detail the process of generating each parameter in our documentation here.\nAudit Results NCC Group performed an audit of the parameter generation code in Summer 2022. No vulnerabilities were found. The detailed public report can be found here. A huge thanks to NCC Group for their many helpful suggestions throughout the audit process!\n","description":"","tags":["cryptography"],"title":"Poseidon377, our instantiation of a SNARK-friendly hash","uri":"/post/poseidon377/"},{"categories":["cryptography","commitment schemes"],"content":"This is a simple explanation of cryptographic commitment schemes, requiring minimal math or cryptography background.\nWhat is a commitment scheme? A commitment scheme is effectively a cryptographic envelope. I can put a secret value in an envelope and then seal the envelope. We call this committing to the secret value. And then I send the envelope to someone or put it somewhere I cannot tamper with it, to open and reveal the secret value I committed to at some later time.\nThe two important properties of this scheme are:\nHiding: No one can see inside the envelope until it is opened. Binding: Once I have commited to this secret value and sent it, I cannot change the value. The commitment has bound me to a specific value. This is an imperfect analogy, but it gets across these two important properties of commitment schemes.\nCryptographic Commitments For cryptographic commitments, we’re going to need two algorithms: Commit and Verify. Let’s go through each one now.\nCommit The Commit algorithm lets us create commitments. Let’s call a commitment com. To commit to a value value:\ncom = Commit(value, randomness)\nwhere randomness is another secret value, but one picked randomly, hence the name. The result com is what we’ll send or broadcast, i.e. com is our sealed envelope.\nVerify When we’re ready to open the commitment, anyone can verify the commitment was opened correctly once they get the values for value and randomness:\nVerify(value, com, randomness) = [accept, reject]\nThe result of the Verify algorithm is either to accept or reject the opening as valid.\nHiding and Binding Remember that we wanted our commitment scheme to share the security properties as the toy example: hiding and binding. For this cryptographic scheme, what does this mean a bit more concretely?\nHiding com should not reveal anything about the committed secret value. You should not be able to see inside the envelope.\nBinding Given a commitment com = Commit(value_1, randomness_1) it should not be possible to find another valid opening, i.e. another value_2 and randomness_2 such that com = Commit(value_2, randomness_2), where we exclude the trivial value_2 != value_1.\nHash Commitments Now let’s turn to some ways to make real cryptographic commitment schemes.\nWe can make a commitment scheme from a collision-resistant hash function H().\nOur commitment will be generated by simply hashing our value and randomness:\ncom = H(value, randomness)\nAnd then we can verify openings using:\nH(value, randomness) == com\nIf H is collision-resistant, then the scheme is binding. Because if you can find a second valid opening, then you’ve found a collision in H and it is not collision-resistant.\nIf H produces random outputs, then you learn nothing about value, so the scheme is hiding.\nPedersen Commitments Given a finite cyclic group, such as an elliptic curve group, we can compute:\ncom = Commit(value, randomness) = [value]G + [randomness]H\nwhere G and H are two points from the elliptic curve group, and value and randomness are scalars.\nWe get another elliptic curve point as a result.\nWe can then verify by checking:\n[value]G + [randomness]H == com\nOne handy feature about Pedersen commitments is that they are homomorphically additive. This means when we add the commitments, we get the commitment of the sum of the values we committed to: all without knowing the secret values.\nLet’s show this. Given two commitments com_1 = Commit(v_1, r_1) and com_2 = Commit(v_2, r_2):\ncom_1 + com_2 = [v_1]G + [r_1]H + [v_2]G + [r_2]H = [v_1 + v_2]G + [r_1 + r_2]H = Commit(v_1 + v_2, r_1 + r_2) We end up with the commitment to the sum of the input secret values.\n","description":"","tags":["cryptography"],"title":"Commitment Schemes 101","uri":"/post/commitments/"},{"categories":["Post","Rust"],"content":"In my last post I described how I implemented the signal-protocol Python library, which provides Python bindings using Pyo3 to an upstream maintained Rust cryptography crate implementing the Signal protocol. I created the the signal-protocol library in order to prototype end-to-end encrypted messaging between journalists and their sources through SecureDrop. In the SecureDrop ecosystem, journalists use a Python project, securedrop-client, hence the need for the Python bindings, and sources use Tor Browser.\nFor the Tor Browser-based client for sources, I needed to either use another implementation of Signal in JavaScript (which does exist), or just write a crate that has the existing upstream cryptography crate as a dependency, and compile it all to WebAssembly. As you can probably tell from the title of this post, I went with the latter approach. It’s honestly pretty cool that I can use the same Rust crypto logic fairly easily for both endpoints, thanks to Pyo3 and WebAssembly.\nIn this brief post we’ll cover:\nan intro to WebAssembly where wasm-bindgen and wasm-pack fit in useful crates you should know of, including js_sys and web_sys helpful references to read more WebAssembly? WebAssembly is a binary-code format that runs in a stack-based virtual machine. It’s supported in all modern browsers, and can also be run in other runtimes (e.g. see WASI) that are browser independent.\nWhy use WebAssembly? Two of the most common reasons are portability and performance. For computationally intensive tasks that need to run in a Browser, you might rewrite the expensive parts in a language that compiles to WebAssembly, leaving the rest of your JavaScript unmodified (similar to C extensions and Python). In the case of Rust compiled to WebAssembly, we also get to benefit from all the safety guarantees that Rust provides at compile time.\nThere are two main approaches to combining Rust and WebAssembly:\nwriting a mix of JavaScript and Rust. Folks typically use wasm-bindgen and wasm-pack to make this easy and autogenerate a lot of the helper JavaScript code for your WebAssembly module. We’ll cover this below. writing only Rust using a project like Yew. Note that you’re not sidestepping the use of JavaScript, it’s just abstracted away from you so you don’t need to write any JavaScript yourself. At the time of writing, using Web APIs (e.g. using web-sys to manipulate the DOM) does require JavaScript, although this may not always be the case1. I haven’t explored the Yew path myself, so we’ll focus on the former path. wasm-bindgen and wasm-pack wasm-bindgen is a handy tool wherein you can simply add the #[wasm_bindgen] attribute to structs and impl blocks to indicate that they should be exposed to JavaScript. For example, SecureDropSourceSession below is a Rust struct I wanted to make available as a JavaScript class:\n1 2 3 4 5 #[wasm_bindgen] pub struct SecureDropSourceSession { store: InMemSignalProtocolStore, pub registration_id: u32, } It encapsulates a private member InMemSignalProtocolStore that methods in our impl block in Rust will use when performing crypto operations in our WebAssembly module.\nNow I can provide methods (to JavaScript) on this struct using an impl block also with the #[wasm_bindgen] attribute2:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #[wasm_bindgen] impl SecureDropSourceSession { pub fn new() -\u003e Result\u003cSecureDropSourceSession, JsValue\u003e { let mut csprng = OsRng; let registration_id: u32 = csprng.gen(); let identity_key = IdentityKeyPair::generate(\u0026mut csprng); // This struct will hold our session, identity, prekey and sender key stores. InMemSignalProtocolStore::new(identity_key, registration_id) .map(|store| SecureDropSourceSession { store, registration_id, }) .map_err(|e| e.to_string().into()) } The Result\u003cT, JsValue\u003e return type is a common pattern that wasm-bindgen will use to throw JavaScript exceptions when the Err variant is returned.\nOnce the WebAssembly module is compiled and loaded, I can now create SecureDropSourceSession objects (now that I’ve implemented SecureDropSourceSession::new) from JavaScript:\n1 var session = SecureDropSourceSession.new(); Building wasm-pack builds your project along with some autogenerated helper JS to a folder called pkg. This is useful if you use a bundler like Webpack since you can simply add the path to your WebAssembly package to your dependencies. This is also useful if you want to publish your WebAssembly package to npm. By publishing to npm, folks using the package will not need the Rust toolchain installed since you’ll be publishing the built Wasm artifact.\nYou can also go the route of not using a bundler, which you can read about in more detail here.\nUseful crates Other useful crates to know of are js_sys and web_sys:\njs_sys lets you call JavaScript functions from your Rust code, such as escape().\nweb_sys provides Web APIs (through JavaScript). For example, you can manipulate the DOM or get access to the WebCrypto API.\nFor debugging, there’s console_error_panic_hook, which lets you add a panic hook that passes panics through to the JavaScript console:\n1 panic::set_hook(Box::new(console_error_panic_hook::hook)); Referencecs This post was a very brief overview. There are great tutorials out there on Rust and WebAssembly, and the main references I found useful while learning enough to implement my project are here:\nRust and WebAssembly book - an introduction to using Rust and WebAssembly together wherein you implement the Game of Life in the browser MDN’s intro to wasm-bindgen and wasm-pack Programming WebAssembly with Rust - a short read covering both WebAssembly in the browser as well as WASI (if you want to run WebAssembly independent of web browsers) the wasm-bindgen book - This is a nice reference text with the details of using wasm-bindgen. Happy hacking!\nSee the Interface Types explainer. ↩︎\nRandom number generation is using the WebCrypto API’s getRandomValues() method under the hood via the rand and getrandom crates. ↩︎\n","description":"","tags":["Rust","WebAssembly"],"title":"Getting started in Rust and WebAssembly","uri":"/post/webassembly/"},{"categories":["Post","Rust"],"content":"This post describes how I approached writing a Python extension in Rust. The post covers:\nwhy one would even want to do this 🙃 the approaches for calling Rust code from Python an overview of how to create a Python module in Rust using PyO3 some tricky parts, e.g. inheritance building and distributing wheels Let’s get started.\nFirst, why do this at all? There are two main reasons:\nTo use Rust libraries that already exist, e.g. cryptography libraries. To do computationally intensive work that will be too slow in Python. Other approaches if this is the main motivation are using a C extension (e.g. as numpy does) or using projects like Cython or numba. For my use case, I had the first reason, I wanted to prototype something using a Rust crate that implemented a cryptographic protocol.\nApproaches There are multiple approaches for calling compiled Rust code from Python, including ctypes, cffi and PyO3. Here we’ll cover the two most popular: cffi (considered easier to use than ctypes) and PyO3.\ncffi You can use extern keyword to allow other languages to call Rust functions.\nFor example this add function is marked as a public external function using the pub extern keywords. The #[no_mangle] attribute just tells the compiler to preserve the readable function name.\n1 2 3 4 #[no_mangle] pub extern fn add(n: i32, m: i32) -\u003e i32 { n + m } One you compile the above, one can then use Python’s cffi library to call the add function. First one must build the library using the cdylib crate type to produce a dynamic library.\nThen, one can load this dynamic library and call the external Rust functions:\n1 2 3 4 5 6 7 ffi = cffi.FFI() ffi.cdef(\"\"\" int add(int, int); \"\"\") adder = ffi.dlopen(location) assert adder.add(2, 2) == 4 You can see this example in full on GitHub here. Read more about Rust FFI here and if you do take the FFI path, you might want to check out the milksnake project for building and distributing wheels.\nPyO3 PyO3 is a very cool project that allows one to define a Python module entirely in Rust. The above example in PyO3 would be:\n1 2 3 4 #[pyfunction] pub fn add(n: i32, m: i32) -\u003e i32 { n + m } And to define the actual Python module:\n1 2 3 4 5 #[pymodule] pub fn adder(py: Python, module: \u0026PyModule) -\u003e PyResult\u003c()\u003e { module.add_wrapped(wrap_pyfunction!(add))?; Ok(()) } This means that now from the Python interpreter we can just do:\n1 2 3 \u003e\u003e\u003e import adder \u003e\u003e\u003e adder.add(2, 3) 5 As we can see, the PyO3 approach is very straightforward. You simply add attributes to structs and functions in Rust to indicate that they should be exposed to Python, and then you write a Rust function to indicate what the top-level functions and classes are for that module.\nThere are also similarly easy-to-use build tools (via setuptools-rust and maturin) to handle the packaging and build process. You can see this example packaged using setuptools-rust on GitHub here.\nIn the rest of this post, I’ll explain more about using PyO3.\nCreating Python Modules with PyO3 The most important attributes to know are:\n#[pyclass]: to expose a Rust struct as a Python class #[pyfunction]: to expose a Rust function as a Python function #[pymethods]: to expose the methods defined in an impl block of a struct with the #[pyclass] attribute as methods on the corresponding Python class #[pymodule]: to expose a collection of structs or functions as a Python module Using these attributes, PyO3 macros will do all the FFI work for you.\nFunctions and methods exposed to Python must have return values that are either native Rust types (that can be converted to PyObject via the ToPyObject trait) or Python object types (e.g. PyDict not dict). See the list of conversions here.\nFunctions that can fail should return PyResult, which is a type alias for Result\u003cT, PyErr\u003e. If the Err variant is returned, an exception will be raised on the Python side. Note that you can also create custom exception types.\nClasses and methods Let’s create an example class, using #[pyclass] and #[pymethods]:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #[pyclass] pub struct Animal { #[pyo3(get)] name: String, #[pyo3(get)] age: u8, hours_since_last_fed: u8, } #[pymethods] impl Animal { #[new] fn new(name: String, age: u8, hours_since_last_fed: u8) -\u003e Self { Animal{ name, age, hours_since_last_fed } } fn feed(\u0026mut self) { self.hours_since_last_fed = 0; } } The #[new] attribute is used for your object constructor and initialization logic in Python (equivalent of Python __new__()).\nIn Python, you’d call doris = Animal('Doris', 2, 0) to use this.\nThe #[pyo3(get)] attribute lets one read doris.name as member attributes. If you want to set attributes also, you can use #[pyo3(get, set)] (which could replace the Animal::feed() method if we wanted to).\nWe can add this class to a new module as follows:\n1 2 3 4 5 #[pymodule] pub fn adder(py: Python, module: \u0026PyModule) -\u003e PyResult\u003c()\u003e { module.add_class::\u003cAnimal\u003e()?; Ok(()) } Some trickier parts of PyO3 The above parts can cover simple projects. Two more advanced topics we’ll cover are inheritance, and magic methods.\nInheritance What if we want to make a subclasses, say, a Lion, that inherits from Animal? Here’s how we do it:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #[pyclass(subclass)] pub struct Animal { #[pyo3(get)] name: String, #[pyo3(get)] age: u8, hours_since_last_fed: u8, } #[pymethods] impl Animal { #[new] fn new(name: String, age: u8, hours_since_last_fed: u8) -\u003e Self { Animal{ name, age, hours_since_last_fed } } fn feed(\u0026mut self) { self.hours_since_last_fed = 0; } } #[pyclass(extends=Animal)] pub struct Lion { #[pyo3(get)] favorite_meat: String, } #[pymethods] impl Lion { #[new] fn new(name: String, age: u8, hours_since_last_fed: u8, favorite_meat: String) -\u003e PyResult\u003c(Self, Animal)\u003e { Ok((Lion{ favorite_meat }, Animal{ name, age, hours_since_last_fed })) } fn roar(\u0026self) -\u003e String { \"ROAR!!!!\".to_string() } } The #[pyclass] annotations indicate the parent (#[pyclass(subclass)]) and child (#[pyclass(extends=Parent)]) classes. The tuple syntax in the return value of the child is a little “trick” intended for ergonomics: you return PyResult\u003c(Child, Parent)\u003e or (Child, Parent). PyO3 will then run Into\u003cPyClassInitializer\u003e on the child, where PyClassInitializer is PyO3’s pyclass initializer.\nMagic methods One might be surprised to find that implementing magic methods doesn’t work in a #[pymethods] impl block. It turns out that you can implement Python “magic” methods like __repr__ and __richcmp__ using the PyObjectProtocol trait and the #[pyproto] attribute in a separate impl block. For example, to add a nice string representation for Animal:\n1 2 3 4 5 6 7 8 9 #[pyproto] impl PyObjectProtocol for Animal { fn __str__(\u0026self) -\u003e PyResult\u003cString\u003e { Ok(String::from(format!( \"Animal: {}\", self.name, ))) } } See some additional examples here and here.\nDistributing wheels We want to build and distribute wheels that do not require the rust toolchain to be installed on target systems. Fortunately, with setuptools-rust and maturin, that’s pretty simple. For setuptools-rust our setup.py for the zoo example would be:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import sys from setuptools import setup from setuptools_rust import Binding, RustExtension setup( name=\"zoo\", version=\"0.0.1\", classifiers=[ \"License :: OSI Approved :: GNU Affero General Public License v3 or later (AGPLv3+)\", \"Development Status :: 3 - Alpha\", \"Intended Audience :: Developers\", \"Programming Language :: Python\", \"Programming Language :: Rust\", ], packages=[\"zoo\"], rust_extensions=[RustExtension(\"zoo.zoo\", \"Cargo.toml\", binding=Binding.PyO3)], setup_requires=[\"setuptools-rust\u003e=0.10.1\", \"wheel\"], zip_safe=False, # Rust extensions are not zip safe ) See the full project here.\nLocally, if we’re on macOS, to build macOS wheels:\npython3 setup.py sdist bdist_wheel To build manylinux wheels we can follow the procedure described in the setuptools-rust project. First we fetch the Python Packaging Authority manylinux image:\ndocker pull quay.io/pypa/manylinux2014_x86_64 Then using the default build-wheels.sh script provided by setuptools-rust:\ndocker run --rm -v `pwd`:/io quay.io/pypa/manylinux2014_x86_64 /io/build-wheels.sh This leaves us with built wheels in dist/ ready for upload to PyPI. And we should just upload the manylinux wheels built by the script as PyPI does not support wheels with platform tags like linux_x86_64 (these are also produced by the above wheel build command but can be discarded).\nFin I hope you’re convinced that writing Rust extensions with PyO3 is approachable. To read more check out the PyO3 guide. If you want to see a larger example, you can check out the library I wrote using PyO3 here and install in Python 3.7+, via pip install signal-protocol 😊 .\n","description":"","tags":["Rust","pyo3"],"title":"Creating Python extensions in Rust using PyO3","uri":"/post/pyo3/"},{"categories":["Post","threat modeling"],"content":"Motivation When performing software-centric threat-modeling on an application1, one typically:\ngenerates at least one Data Flow Diagram (DFD) or other diagrams that model the software, enumerates threats using the diagram(s) as an aid, and then determines which mitigations should be applied. Automated tools can potentially aid the threat modeler in each of these stages. When the design of a system is modified, the threat modeling exercise may be performed again, resulting in needing to pass through the above steps again. Even if a design remains static, the threats and mitigations should still be evaluated periodically as the threat landscape may evolve, e.g. if attacker capabilities or motivations change. This means that tools or processes that can aid the threat modeler perform these tasks on a continuous basis can be of great value.\nTooling to aid these steps When going about the above steps, several questions immediately come up, some of which have existing solutions in the ecosystem, and some of which do not.\nHow to generate and store the diagrams? For an initial iteration, taking a picture of a sketch or whiteboard drawing can be sufficient. On an continuous basis having the DFD elements and data flows stored in a version-controllable format like XML (as draw.io does), YAML (as threagile or threat-modeling does) or directly as code (as pytm does) is ideal. All of these tools enable one to generate the data flow diagrams from the version controlled DFD data.\nSome tools like OWASP Threat Dragon, Microsoft’s TMT and draw.io also provide a GUI to aid in this step.\nHow to enumerate and store threats? Some of the above mentioned tools provide assistance for the threat enumeration process, using an existing methodology like STRIDE-per-element (in the case of Microsoft TMT and Threat Dragon), although these can only be used as an aid and need to be critically evaluated by the threat modeler to determine their relevance, and to add other potentially relevant threats. The threat modeling may also use existing attack libraries (like MITRE CAPEC) to aid in threat enumeration.\nIn terms of storing the threats once enumerated, a first-order approach is to use an existing ticketing system (e.g. a private GitHub repository) or a spreadsheet to store enumerated threats.\nThe advantage of a ticketing system is that it’s easy to crosslink with mitigations or individual development tickets being implemented. The downside is that it’s not easy (as it is with a spreadsheet) to e.g. sort all threats by risk score, which you might do in order to determine which are the most important threats on which to focus your engineering effort.\nThe downside of the spreadsheet approach is that it’s a bit awkward to perform deeper analysis on, as well as follow one-to-many relationships. But, the spreadsheet has the advantage of being something that one can export as CSV and store in version control (although not in the most readable format, but one could then load the CSV in e.g. pandas if one wants to perform further analysis).\nSome tools like threagile, threat-modeling, and pytm store the enumerated threats in YAML, JSON, and in the case of pytm, a SQL dump. These all enable easy analysis using other tools as well as for YAML and JSON human-readable storage in version control.\nHow to determine which mitigations to apply? Once threats have been enumerated, next we need to decide how to make decisions about them. If we can mitigate all threats, then great. However, we might be unable to completely mitigate all threats, so we need to decide how to allocate the engineering effort we have in a rational manner to manage the risk. There are not a lot of tools that aid in this dimension (see the next section on Simulation-like question answering).\nEdit 11/12/2020: Another important aspect here is how best to store the mitigations - and their mapping to threats. It is highly useful to track which mitigations map to which threats such that developers can clearly see the purpose of the mitigation, and to enable one to evaluate the impact of removing a given security control (e.g. if debates arise on their importance due to maintenance burden or user impact). In threat-modeling, the approach is to store the mitigations also in YAML, similar to the threats, with references to the threat IDs they apply to, to enable this sort of analysis2.\nIdeas Next, here are some directions that I think would be useful to explore further.\nIntegration in CI/CD pipelines The threatspec folks suggest integrating their tool (which is quite cool and involves adding annotations directly in the application code) as part of a CI/CD pipeline performing report or DFD generation.\nOne could also imagine deeper integration in CI/CD pipelines. For example, by parsing the DFD, threats, and mitigations data, in CI one could flag issues like:\nDataflow has no threats enumerated for it (i.e. threats are missing) Threats have no status (i.e. a decision needs to be made to accept, transfer, mitigate, etc.) Or inconsistencies in the DFD data itself to guide system modelers, e.g.:\nNo trust boundaries present (there should be at least one trust boundary) Process element lacks entry or exit dataflow (there should be an entry and an exit) This is explored in the threat-modeling tool (ticket).\nSimulation-like question answering As a system designer we often ask questions like (see ticket here):\nWhat happens if I remove mitigation X? How much does total risk increase? I only have time to implement one complex mitigation, is it rational to implement mitigation X or mitigation Y? A tool that can aid in answering these questions (in a lightweight fashion), e.g. by recomputing the total risk score when a mitigation is removed or added, could be very helpful in decision-making. Of course a pre-requisite here is the accuracy of the risk scores - if they are very uncertain, that may need to be factored into the analysis (e.g. as is done in FAIR (Factor Analysis of Information Risk) risk framework).\nThe only tool I’m aware of in this direction is the SPARTA tool from KU Leuven which allows one to both rank risks as well as answer some simulation-type questions using an approach based on the FAIR risk framework.\nAttack tree (or attack graph) generation While the exercise of generating attack trees manually is a useful one for system designers (as is threat enumeration), one could also imagine at least partially automating the generation of attack trees.\nWhen an attacker is able to successfully realize a given threat as an attack, other threats become accessible, e.g. once one is able to get code execution in a VM, they can next attempt to exploit a bug in the hypervisor. One could annotate threats with these child-parent relationships to indicate which threats become accessible once the given threat is realized, and from that derive attack trees. This is the approach taken here in the threat-modeling tool3.\nFin If you have thoughts on this or know of tools or approaches (especially if FLOSS 😇) that address the above concerns, feel free to drop me a note on Twitter or by email.\nA lot has already been written on the benefits of doing threat modeling so I’m not going to espouse the benefits here. ↩︎\nArmed with this mitigations data, one could do a “resilience” sort of test here, where one could simulate the impact of the failure of a given countermeasure: pick a given countermeasure, disable it, what is the impact of disabling this mitigation? If it’s significant, then this shows an area where one should focus additional effort to provide defense in depth. ↩︎\nOne could also imagine various ways of removing the parent-child threat annotation requirement, e.g. perhaps one could have the system designer specify the likely entry-point threats on the attack surface of the system. At this point provided there is a mapping of threats to DFD elements, the rest of the process could be automated: the entry-point threats are root nodes, and then one follows the dataflows to grow the rest of the attack tree, adding a child node in the attack tree for every threat associated with the destination node. ↩︎\n","description":"","tags":["security","threat modeling"],"title":"Continuous threat modeling, Part 1: Tooling wish list","uri":"/post/continuous-threat-modeling/"},{"categories":["Post","Signal"],"content":"Next in the series, I investigate current messaging applications that both provide web applications and are using the Signal Protocol (or a protocol very similar or derived from Signal), here specifically Wire and Whatsapp. I’m not looking into the voice and video aspects, just the messaging and file sharing capabilities as I’m investigating to see how a similar approach could be used for SecureDrop, where voice/video isn’t an option. As always, if you have thoughts on this or notice errors, feel free to drop me a note on Twitter or by email.\nEnd-to-end over the web A lot has been written already on the significant challenges using a web browser for e2e crypto.\nThe main issue is that when you use an e2e webapp, the webserver you’re connected to is serving the cryptographic code to you. This means a malicious server can tamper with any of the code served to you and can, for example, display “verified fingerprints” in the webapp while an active man-in-the-middle is taking place.\nCompare this with developer-signed desktop applications, where you can ensure that code from the developer you trust is what is running. In the desktop application case, if the server you’re connected to is malicious they can’t replace the code running on your desktop.\nBut for e2e webapps, you’re trusting the server - and the integrity of the data flow between your browser and the server.\nAnother challenge is you need to decide where to store the key material. One could put it in browser storage, but what happens if a user changes browsers? Do you generate new keys each time a user uses a new browser? Or do you put the key material somewhere else on the user’s machine? At least for Wire, they do generate new keys when you use a new browser, which we’ll see more below.\nWire The below information is from reading the Wire Whitepaper.\nWire has apps for mobile (Android, iOS), desktop (macOS, Windows, Linux), as well as an end-to-end encrypted web application.\nUser and client registration First, users must have a Wire account to use the service. User registration can be done via:\nemail, where the user must validate a random verification code sent via email (to prove ownership of the account); these must be verified in three attempts, phone, where the user must validate a verification code sent via SMS, this code must also be verified in three attempts, User registration gets the user their Wire internal ID (UUID v4) and an authentication cookie (used to authenticate to the Wire API).\nNext, users can register clients in order to start sending/receiving e2e messages. Wire allows a single temporary client and up to 7 permanent devices. During client registration, the client generates a Curve25519 long-term identity keypair and sends:\nwhat type it is (either Permanent or Temporary), the public Curve25519 identity key 65,535 one-time use Curve25519 prekeys (see Blog post 1 on X3DH for how that works), which clients periodically replenish, and the “last resort” (lr) prekey, which is similar to the signed prekey in X3DH, in that it’s used if all other prekeys are exhausted. The Wire whitepaper makes no mention of signing this prekey though. Encryption The primitives Wire uses are ChaCha20, HMAC-SHA256, and Curve25519, with HKDF for key derivation.\nSenders encrypt a message to all members of a group (“conversation” in Wire) for every participant and send in a batch to the server. If the sender did not include messages for all participants, the server rejects the batch. Clients keep track of which participants are in a conversation, and learn from the server when they need to update. This is more similiar to how Sesame works in Signal, compared to Signal groups v1, as the server is keeping track of group participants, which clients update to.\nFor large file attachments, senders generate a symmetric key and encrypt the file using AES-CBC-256 with PKCS#5 padding. The key and SHA-256 of the ciphertext are then encrypted for each participant. The servers gets just one copy of the encrypted file and a message as usual for each participant. This means that clients do not need to upload the same potentially large file many times encrypted to different participants. According to the whitepaper, these assets are stored basically indefinitely (as opposed to when all clients have downloaded the attachment).\nFingerprint Verification Users can compare fingerprints, and if all clients in a conversation are verified, this in indicated in the UI with a blue shield.\nWeb Client New “devices” (i.e. new browsers) generate new device keys, the private key material of which is stored using IndexedDB. Messages are also stored only locally once fetched from the server, which is why conversation history does not appear when you sign in using a new device.\nThe use of IndexedDB is why Wire does not work in Tor Browser - IndexedDB does not work in PBM (Private Browsing Mode) (Tor ticket, Firefox ticket 1). This makes sense since IndexedDB adheres to a same-origin policy and as such stores data associated with the origin, so allowing this storage in PBM would present privacy issues. However, there are some proposals on the Mozilla side to resolve this and thus enable IndexedDB storage in PBM, so this is an area to monitor.\nWhatsApp The below information is from reading the WhatsApp Security Whitepaper.\nWhatsApp provides clients for mobile (Android, iOS), desktop (macOS, Windows), as well as a web client via WhatsApp Web. I couldn’t really find too much detail about how the web client was approached from a security standpoint, but the below information should apply to all clients.\nUser and client registration During client registration, the process works as one would expect from reading the X3DH specification: clients generate a long-term identity key, a signed prekey, and one-time pre-keys. The client then sends the public parts of all the above keys along with the signature of (signed) prekey to the WhatsApp server.\nClients authenticate to servers using their long-term keypair, so there are no user credentials on the server to compromise.\nEncryption To establish sessions, the initiator computes a shared key ($SK$ in the X3DH specification), which the receiver also constructs on receipt of the initiating message. They both use HKDF to derive an initial root key and chain key to initialize session state. From that point clients can encrypt and decrypt messages by deriving each individual message key from chain keys, ratcheting chain keys forward, and updating root keys using updated shared secrets. This is all performed as described in the double ratchet specification.\nFor large attachments, WhatsApp has a similar blob store as Wire. They have senders generate an ephemeral 32-byte AES encryption key (for AES-CBC with random IV) and an ephemeral 32-byte MAC key (for HMAC-SHA256). They encrypt-then-MAC and upload the attachment. The message to the recipient then just contains the AES key, the IV, the MAC key, a SHA256 hash, and a reference to the location in the blob store. Recipients must download, decrypt, and verify the hash and MAC prior to decryption. As above, this prevents clients from having to encrypt and send the same large file to multiple participants.\nIn terms of groups, WhatsApp uses a method that requires servers have knowledge of group participants. This is because Whatsapp uses a “server-side fan out”, wherein a message to N clients is just sent once by the client, then forwarded on to the N clients by the server. The disadvantage here is that the server must know which N participants should receive the message. The advantage is that one could have larger groups than if you require a client-side fan out, wherein all clients send each message to all N participants in a group.\nIn more detail, WhatsApp has clients generate a random Chain key and random Curve25519 key pair they call the Signature key upon joining a group. These two keys are described as a Sender key, and are sent to all group participants pairwise. Each chain key is used by participants to derive individual message keys. Ciphertexts are signed by senders using the signature key. When a group participant sends a message, the server performs the server-side fan out, sending the ciphertext to all other group participants. The receiving group participants can decrypt the ciphertext since they received the chain key from the sender previously. The sender keys are rotated whenever a group member leaves. One hiccup here is that as described, the DH ratchet is not happening: if a chain key is compromised, all other messages using message keys dervied from that chain key can be decrypted. This will be the case until a group member leaves.\nOne other interesting note from the whitepaper is that client-server connections are also encrypted at the transport-level using Noise Pipes (they used Curve25519, AES-GCM, SHA256).\nFingerprint Verification Fingerprints are in the form of QR codes or 60-digit fingerprints (30 digits per party). These codes are derived from the public part of the long-term identity key for both parties, along with for the QR code, a version and user identifiers for each party.\n","description":"","tags":["Signal","security","protocol"],"title":"Investigating the Signal Protocol, Part 3: Web applications","uri":"/post/signal-webapps/"},{"categories":["Post","Signal"],"content":"Next in the series, I investigate how groups and multi-device support are handled. If you have thoughts on this or notice any errors, feel free to drop me a note on Twitter or by email.\nGroups Signal allows private groups where the server doesn’t have access to the group metadata, including the list of members in each group. Servers cannot even distinguish group from direct messages.\nLet’s say a group contains two users, Alice👧🏼 and Bob👦🏽, and a third user Jen🧙 wants to send the group a message. Alice👧🏼 adds Jen🧙 to the group, and then Jen🧙 can send the group a message by sending messages individually to each user in the group: she encrypts her message $M$ to each group participant separately, encrypting using the next message key she has for Alice👧🏼 and Bob👦🏽. One caveat is that she uses separate ratchet state than her direct chats with Alice👧🏼 and Bob👦🏽.\nGroup management messages are regular Signal (i.e. e2e encrypted) messages that only clients can decrypt and act on. The advantage of this approach is that the server stores nothing about groups, although the server can of course infer group participants through observing the timing and metadata of messages, but we’ll put that aside for now.\nFor example, if Alice👧🏼 chooses to leave a group:\nAlice👧🏼 sends a leave group management message to all other group members (along with the ID of the group). Clients process this and remove Alice👧🏼 from their locally stored list of users in that group. Similarly for other group information update tasks, any group participant can send a group management message adding (but not deleting) new users or updating the group information. This means that since users must remove themselves from groups using a leave message, if a group wants to remove a given user and the user does not leave, effectively users must create a new group without the participant they want to remove.\nThe group system is under active development, see the preview version of the group management using anonymous credentials described here. Note also that Whatsapp appears to handle groups differently from the Signal application.\nMulti-device The full description is covered in this specification.\nFor multi-device support, the main concepts we need to know about are:\nUsers, which are identified by UserID and can have one or more devices. Devices, which are identified by DeviceID. Each device has its own keypair. Sessions, which are identified by SessionID. This is the ratchet state that can be used for processing incoming and outgoing messages. The server keeps track of the mapping of users (UserID) to devices (DeviceID). This is different than with groups, where the server doesn’t need to keep track of users in groups.\nAn initiation message begins a session. In this message, the sender includes their device public key, so the recipient knows which of the user’s devices is active. Clients keep track locally of a mapping of users (UserID) to devices (DeviceID) and furthermore, which of the devices is active.\nWhen Bob👦🏽 sends Alice👧🏼 a message $m$, he encrypts $m$ for all of Alice👧🏼’s devices using active sessions if available. This is the first batch of messages. Bob👦🏽 does the same action for himself: he encrypts the message $m$ to all of his own devices (a second batch of messages). For each batch he:\nSends the batch of messages to the server. If Bob👦🏽’s local view of the receiver device state was stale, the server rejects the messages and lets Bob👦🏽 know to update. Else, the servers puts the messages in the message delivery queues for those devices. By doing this Bob👦🏽’s other devices will also have the message history of this conversation even if the device is not active.\nWhen Alice👧🏼 connects to the server to get her messages, she uses any initiation messages to update her local store of device and session state. Otherwise, she uses the existing session state as usual to process the message.\n","description":"","tags":["Signal","security","protocol"],"title":"Investigating the Signal Protocol, Part 2: Groups, devices","uri":"/post/signal-group-multidevice/"},{"categories":["Post","Signal"],"content":"I’ve been investigating applications that use the Signal Protocol in order to determine if the Signal Protocol for asynchronous messaging might be appropriate for use for applying to SecureDrop messaging in the future. In this post are some notes from reading the Signal Protocol specifications, which I thought might be a useful reference and explanation for others. If you notice an error, or have other thoughts on anything here, feel free to drop me a note on Twitter or by email.\nThe protocol consists of two main parts, one for establishing key agreement between two parties, and another for “ratcheting” or deriving new ephemeral keys from that initial key material.\nKey Agreement using Extended Triple Diffie-Hellman (X3DH) This is the process that occurs on first-time messages.\nThe full description is covered in this specification. I use the same nomenclature as the specificiation for ease of comparison.\nX3DH is used in order to set up a shared secret between two parties. In this scenario we have a a server which is where we’ll store information in case either party is offline. We also have our two users:\nAlice(👧🏼) who is online. Bob(👦🏽) who is offline. But Bob has helpfully published some data to the server for Alice to use to send him secure messages while he’s offline. Alice👧🏼 and Bob👦🏽 will generate several elliptic curve key pairs using either Curve25519 or Curve448. How these curves can be used in Diffie-Hellman key exchange is described in RFC 7748, Section 6.\nFor Alice👧🏼, she has the following public keys:\nlong-term identity public key $IK_A$ emphemeral public key $EK_A$ Bob👦🏽, who recall is offline, has published the following public keys to the server:\nlong-term identity public key $IK_B$ signed public prekey $SPK_B$. Bob👦🏽 will publish new signed prekeys from time to time, which will replace the old one. He obviously publishes both the prekey and the corresponding signature (with his long term identity key). When Bob👦🏽 replaces a prekey, he’ll want to delete the private key of the old keypair after waiting a period of time to allow for recently sent messages to be delivered. $n$ one-time public prekeys $OPK^{1}_{B}$…$OPK_{B}^{n}$. Since these are one-time use, these will eventually run low (especially if Bob👦🏽 here is a popular fellow) so occasionally Bob will upload additional prekeys. When Bob receives a message using a public prekey, he’ll use the private key to process the message, and then delete the corresponding private key. When Alice👧🏼 wants to send an initial message, she:\nFetches Bob👦🏽’s long-term identity public key. Fetches Bob👦🏽’s signed public prekey and the signature. She verifies the signature (and stops if the verification fails). She fetches one of Bob👦🏽’s one-time public prekeys ($OPK^{1}_{B}$) - if one is available. Else she skips this step. These items are found in a PreKeyBundle.\nNext, four Diffie-Hellman (DH) shared secrets are derived using:\nAlice👧🏼’s long term identity key $IK_A$ and Bob👦🏽’s signed pre-key $SPK_B$. Alice👧🏼’s emphemeral key $EK_A$ and Bob👦🏽’s long term identity key $IK_B$. Alice👧🏼’s emphemeral key $EK_A$ and Bob👦🏽’s signed pre-key $SPK_B$. Alice👧🏼’s emphemeral key $EK_A$ and Bob👦🏽’s one-time public prekeys $OPK^{1}_{B}$. Since the private key material for DH secrets 3-4 above will be deleted after use, these provide forward secrecy. This also means that in the future if an attacker collecting ciphertexts is able to compromise Alice👧🏼’s long-term identity key, the attacker cannot recover all four DH shared secrets since the ephmeral key material is long gone, thus they are unable to decrypt the ciphertexts encrypted using secrets derived from these DH secrets. By using the long-term identity keys - which can be verified using manual verification of safety numbers - in steps 1-2, these steps mutually authenticate Bob👦🏽 and Alice👧🏼.\nNext, DH outputs 1-3 (and 4 if available) are concatenated and used as an input for HKDF, an HMAC-based Key Derivation Function (KDF). A KDF does what it sounds like: takes some input and produces cryptographically strong key material. HKDF is defined in RFC 5869. In our protocol, the output of HKDF is a shared key $SK$! These three (and sometimes four) DH key exchanges give the protocol its name.\nAt this point, Alice👧🏼 can now send a message to Bob👦🏽. She sends him $IK_A$, $EK_A$, the ID of the one-time prekey she used ($OPK_{B1}$) (Bob👦🏽 will delete the corresponding private key material once he processes the message), and the ciphertext of her message encrypted using the shared key $SK$, which Bob👦🏽 can also derive.\nImplications An attacker that is able to compromise the long-term identity key can masquerade as the user. They can sign prekeys and create new sessions. But, provided an attacker does not have access to previous ephemeral prekey (i.e. private) key material - which are deleted in the protocol after use - the attacker will not be able to reconstruct prior $SK$ and thus decrypt previous ciphertexts. If the private keys corresponding to currently uploaded prekeys, either one-time or signed, were compromised, they should be replaced.\nThe specification also states that rate limits should be in place for getting a one-time prekey: this prevents an attacker from exhausting one-time prekeys, which would force Alice👧🏼 to fall back to using only Bob👦🏽’s signed prekey.\nDouble Ratchet Algorithm At this point once the initial shared secret is established, the “ratchet” comes into play.\nThe full description is covered in this specification in the Double Ratchet without header encryption section. I use the same nomenclature as the specificiation for ease of comparison.\nKDF chain A KDF chain is when a key and some additional input is used as input to a KDF, producting key material, some of which is used as a new KDF key, and some of which is used as an output key. The output keys are used, and the next step in the KDF chain uses the new KDF key as an input. Each step of the KDF chain looks like the following:\nSymmetric-key ratchet A “symmetric ratchet” is a KDF chain that is used to derive per-message keys. Signal’s “Chain key” refers to the KDF key for each of the symmetric-key chains.\nA single step in the symmetric key ratchet looks like the following:\nDH Ratchet The DH ratchet is the process by which chain keys in the symmetric ratchet are updated.\nEach party has a ratchet key pair, which is a public-private Diffie-Hellman key pair.\nWe observed in the X3DH protocol that in the first message Alice👧🏼 sent, she included the public part of her emphemeral key $EK_A$ such that bob could derive the same shared secret $SK$.\nIn subsequent messages, Alice👧🏼 (and Bob👦🏽) can advertise new public keys (new “ratchet” public keys), which when Bob👦🏽 (and Alice👧🏼) receives he can use to construct new DH ratchet shared secrets using the local corresponding ratchet private key. Alice👧🏼 and Bob👦🏽 take turns ratcheting the DH secrets forward. Senders must include the sender ratchet key in each Signal message.\nSignal Protocol Putting this together, Alice👧🏼 and Bob👦🏽 each have:\na DH ratchet a root (symmetric-key) chain a sending (symmetric-key) chain a receiving (symmetric-key) chain Alice👧🏼’s sending chain and Bob👦🏽’s receiving chain are the same, similarly Alice👧🏼’s receiving chain and Bob👦🏽’s sending chain also are the same. Output keys from the sending and receiving chains are used for individual message encryption and decryption.\nOnce a message key is used (i.e. for encryption or deletion), it is deleted by clients. If messages are delivered out of order, the receiver can just ratchet the chain forward to get the key material for the most recent delivered message, and store the message keys from the previous steps until they are delivered.\nThe root chain takes as input DH secrets from the DH ratchet (derived as described in the prior section). The output keys from the root chain are new chain keys for the sending and receiving chains. As stated above, the message keys from those sending and receiving chains are used to encrypt and decrypt individual messages.\nThe initial root key for the Double Ratchet protocol is the SK generated from the X3DH protocol. Initially $SPK_{B}$ becomes Bob👦🏽’s initial ratchet keypair.\nProperties In summary, in addition to protecting the confidentiality of messages, some other useful properties of the above protocol are:\nDeniability - anyone can claim a message came from one of the participants at the end of a conversation. Forward secrecy - If long-term keys are compromised, prior messages cannot be decrypted. The key material to decrypt them is ephemeral and will have been deleted. Self-healing and “future secrecy” - If a key is compromised, the protocol heals, meaning that future communications will return to a state unknown to the attacker. This is done by updating chain keys using the DH ratchet. Authentication - If key fingerprints are mutually verified, the protocol provides end-to-end authentication. ","description":"","tags":["Signal","security","protocol","cryptography"],"title":"Investigating the Signal Protocol, Part 1: Foundations","uri":"/post/signal-protocol/"},{"categories":["Post","Tor"],"content":"Secure The News (STN) is a project by Freedom of the Press Foundation to track and advocate for security and privacy technologies in news organizations. I’ve been working a bit on expanding the scope of STN from its original goal, HTTPS adoption, to how news sites treat Tor users.\nThe first expansion is to scan for onion service availability using the the Onion-Location header (or its presence in a \u003cmeta\u003e tag in the site’s page content). This is used by Tor Browser to alert users to the presence of an onion service.\n📈 Leaderboard: https://securethe.news/\n📝 Blog announcement: https://freedom.press/news/onions-side-tracking-tor-availability-reader-privacy-major-news-sites/\n🐿️ Code: https://github.com/freedomofpress/securethenews\n","description":"","tags":["onion services","security"],"title":"Scanning for onion service availability","uri":"/post/onion-available/"},{"categories":["Post"],"content":"Being able to reproducibly build binary artifacts means that users, developers, and others can agree that the shipped artifact was correctly built from the source code (that one can inspect), and no intentional or unintentional malicious code was introduced during the build process.\nOne hiccup we’ve encountered in SecureDrop development is that not all Python wheels can be built reproducibly. We ship multiple (Python) projects in debian packages, with Python dependencies included in those packages as wheels. In order for our debian packages to be reproducible, we need that wheel build process to also be reproducible. That wheel process is reproducible (as of pip wheel 0.27.0 - see relevant issue) if you set SOURCE_DATE_EPOCH to be a constant value. However, there are still sources of nondeterminism for some projects.\nFor our purposes, this has resulted in our building the wheels (once), saving those wheels on a pip mirror, and then using those wheels at debian package build time. A few times, we’ve asked “wait, which wheels can’t be reproducibly built again?”. So I made a little tracker on https://reproduciblewheels.com/ for convenience.\nEDIT: As of August 19, 2020, passing a static --build directory to the pip wheel command below means that all currently tracked wheels are reproducible 🎉.\nHow it works I first selected the 100 most popular packages on PyPI in the past year plus any dependencies that are on FPF’s pip mirror1.\nThen, I have a little function that builds the wheel twice and then compares the SHA256 hash to determine if they are the same. The build command is:\n1 python3 -m pip wheel \u003cproject_name\u003e --no-binary :all: --no-cache-dir Here --no-binary :all: is used to ensure that I download the source tarball and --no-cache-dir is used so that I don’t inadvertently use a cached built artifact.\nA friendly bot is running the above build function nightly for every monitored project, saving the results as JSON, and then updating the static HTML, which is deployed when it’s committed to the main branch via GitHub pages. That’s it!\nIf you find issues or think it should be tracking something else, just open an issue.\nI should note here that from this set I excluded a few (7) projects that either required additional build requirements that I didn’t have out of the box in the build environment or had some other build-time issue (for the interested, this is ticket #2 on the bugtracker). ↩︎\n","description":"","tags":["Python","wheels","reproducible builds"],"title":"Tracking which wheels can be reproducibly built","uri":"/post/reproducible-wheels/"},{"categories":["Post","Software Development"],"content":"Earlier this year at USENIX Enigma, I presented some recent work towards rearchitecting the SecureDrop journalist experience.\nIn SecureDrop we currently use an airgapped workstation for viewing documents to reduce the impact of malware present in malicious documents. This presents challenges in terms of maintenance, usability, and even security (i.e. airgaps don’t receive automatic updates).\nThe basic idea of the rearchitecture of the journalist experience is to replace SecureDrop’s multi-machine airgap with a single machine using Qubes (Xen) for compartmentalization. This enables journalists to work with source materials in a secure environment, while still protecting them in the event of a system compromise. Check out the full talk here:\nAfter the talk, I decided to write up basically the above talk describing the design of the system in an expanded form in a whitepaper. Check out the full whitepaper here (PDF).\n","description":"","tags":["SecureDrop","qubes","whistleblowing","whitepaper"],"title":"Protecting journalists from malware using QubesOS","uri":"/post/securedrop-qubes-workstation/"},{"categories":["Post"],"content":"A fun small project I worked on earlier this year was creating an HTTPS Everywhere custom ruleset channel for the SecureDrop project. HTTPS Everywhere is a popular browser extension maintained by EFF that rewrites HTTP URLs to HTTPS when possible. A set of rules is used to determine how URLs should be rewritten. HTTPS Everywhere is included in Tor Browser to reduce the impact of malicious exit nodes.\nThis is relevant for SecureDrop, because each news organization advertises an onion URL that corresponds to the web application (“source interface”) where a source should upload documents. These URLs are not human readable: an example URL v2 onion services is yyhws9optuwiwsns.onion. With v3 onion services, they are even longer: l5satjgud6gucryazcyvyvhuxhr74u6ygigiuyixe3a6ysis67ororad.onion.\nThe HTTPS Everywhere folks suggested that we could use the custom ruleset functionality in HTTPS Everywhere to create a list of rules rewriting human-readable names to the onion URLs for SecureDrop source interface. This can be scoped to a namespace such that this custom ruleset can only rewrite URLs that end in e.g. securedrop.tor.onion.\nHere’s a little gif showing the rewriting:\nWe created a ruleset channel with organizations that opted-in, allowing them to rewrite e.g. lucyparsonslabs.securedrop.tor.onion to the underlying onion URL. Check the repository out if you’re curious to see the logic we use to maintain and update the rules. Our ruleset channel was included in stable Tor Browser beginning with the 9.5 release.\n","description":"","tags":["SecureDrop","tor","naming"],"title":"Using HTTPS Everywhere rules for SecureDrop onion names","uri":"/post/securedrop-httpse/"},{"categories":["Post","Software Development"],"content":"A queue retrives items in FIFO (first in, first out) order. A priority queue retrieves item based on priority, higher priority items come first. Well, what happens if you submit items that have equal priorities? It depends on how the priority queue was implemented. Read on for how this is handled in the Python standard library’s queue.PriorityQueue.\nLet’s see queue.PriorityQueue in action in a simple case:\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u003e\u003e\u003e from queue import PriorityQueue \u003e\u003e\u003e q = PriorityQueue() \u003e\u003e\u003e q.put((1, 'A')) \u003e\u003e\u003e q.put((2, 'B')) \u003e\u003e\u003e q.put((3, 'C')) \u003e\u003e\u003e q.get() (1, 'A') \u003e\u003e\u003e q.get() (2, 'B') \u003e\u003e\u003e q.get() (3, 'C') \u003e\u003e\u003e q.empty() True As we can see, we’ve put (priority_number, data) into the queue, and retrieved them using the get() method. We see that lower numbers correspond to higher priorities.\nLet’s now add some jobs with equal priorities and retrieve them:\n1 2 3 4 5 6 \u003e\u003e\u003e q.put((1, 'My first job')) \u003e\u003e\u003e q.put((1, 'Another job')) \u003e\u003e\u003e q.get() (1, 'Another job') \u003e\u003e\u003e q.get() (1, 'My first job') We did not retrieve items in FIFO order for jobs of equal priority: 'Another job' was fetched prior to 'My first job' even though it was added afterwards. Why does this happen?\nUsing a min-heap for queue.PriorityQueue The short version is that we grabbed 'Another job' first, because 'Another job' \u003c 'My first job' alphabetically.\nThe longer version is that under the hood, queue.PriorityQueue is implemented using heapq, Python’s heap implementation. Every job is an element in a min-heap. A min-heap is a complete binary tree that satisfies the min-heap propety: the value of each node is greater than or equal to the value of its parent. The root element will be the node with the minimum value. So to get the next job we want to run, we just grab the element at the top of the min-heap, which due to the min-heap property, we know will be the job with the minimum priority value - which remember from above corresponds to the higher priority.\nBut where is this comparison done: 'Another job' \u003c 'My first job'? During heap operations, elements are compared with one another (and swapped if needed). In Python, this is done using the rich comparison operator __lt__. 'Another job' will bubble to the top of the heap since 'Another job' \u003c 'My first job'.\nHow we can solve this Here’s an approach I used for Python 3.5 (the version of Python I was writing for when I looked into using this functionality). For the application I was working on, I needed to retrieve items based on priority, and for items of equal priority, I needed to retrieve items in FIFO order.\nOne simple approach if you hit this problem is following a suggestion in the heapq documentation: “store entries as 3-element list including the priority, an entry count, and the task”, where the entry count is a tie-breaker for jobs of equal priority. Let’s see that demonstrated:\n1 2 3 4 5 6 \u003e\u003e\u003e q.put((1, 1, 'My next job')) \u003e\u003e\u003e q.put((1, 2, 'Another job')) \u003e\u003e\u003e q.get() (1, 1, 'My next job') \u003e\u003e\u003e q.get() (1, 2, 'Another job') In my situation, I was working in a codebase that had already a mediator interface to submit jobs (to queue.PriorityQueue), and job objects themselves were separate objects (i.e. not simple strings in the real applicatoin). I ended up making jobs sortable using the following superclass that implemented the __lt__ rich comparison method:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from typing import TypeVar QueueJobType = TypeVar('QueueJobType', bound='QueueJob') class QueueJob(): def __init__(self, order_number: int, task: str) -\u003e None: self.order_number = order_number self.task = task def __lt__(self, other: QueueJobType) -\u003e bool: ''' We need to use the order_number key to break ties to ensure that objects are retrieved in FIFO order. ''' return self.order_number \u003c other.order_number def __repr__(self) -\u003e str: return self.task When I submitted jobs, I’d do so using an interface like this (application simplified for demonstration purposes, more context is here) that would set the order_number such that it would be monotonically increasing:\n1 2 3 4 5 6 7 8 9 10 11 12 13 import itertools import queue class App(): def __init__(self): self.order_number = itertools.count() self.queue = queue.PriorityQueue() def add_task(self, priority: int, task: str): current_order_number = next(self.order_number) task = QueueJob(current_order_number, task) self.queue.put((priority, task)) Let’s see if jobs with equal priorities are retrieved in FIFO order:\n1 2 3 4 5 6 7 8 \u003e\u003e\u003e from stuff import App \u003e\u003e\u003e app = App() \u003e\u003e\u003e app.add_task(1, 'My first job') \u003e\u003e\u003e app.add_task(1, 'Another job') \u003e\u003e\u003e app.queue.get() (1, My first job) \u003e\u003e\u003e app.queue.get() (1, Another job) Jobs with equal priorities are retrieved in FIFO order, which is what we wanted.\n","description":"","tags":["data structures","queue","python"],"title":"Handling equal priority jobs using queue.PriorityQueue","uri":"/post/priority-queue/"},{"categories":["Post","Software Development"],"content":"When testing codepaths that generate images, one might want to ensure that the generated image is what is expected. Matplotlib has a nice decorator @image_comparison that can be applied for this purpose, but looking at the implementation, it’s pretty tied to the matplotlib Figure object. I wanted something generic to use with PNGs.\nI ended up writing a pytest fixture that would compare the image generated during the test with a baseline image (in tests/baseline_images as in the matplotlib implementatino). Here are the contents of conftest.py, which contain the fixture and its related image similarity assert:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import os import pytest import numpy as np from PIL import Image def assert_images_equal(image_1: str, image_2: str): img1 = Image.open(image_1) img2 = Image.open(image_2) # Convert to same mode and size for comparison img2 = img2.convert(img1.mode) img2 = img2.resize(img1.size) sum_sq_diff = np.sum((np.asarray(img1).astype('float') - np.asarray(img2).astype('float'))**2) if sum_sq_diff == 0: # Images are exactly the same pass else: normalized_sum_sq_diff = sum_sq_diff / np.sqrt(sum_sq_diff) assert normalized_sum_sq_diff \u003c 0.001 @pytest.fixture def image_similarity(request, tmpdir): testname = request.node.name filename = \"{}.png\".format(testname) generated_file = os.path.join(str(tmpdir), \"{}.png\".format(testname)) yield {'filename': generated_file} assert_images_equal(\"tests/baseline_images/{}.png\".format(testname), generated_file) The assert rescales the images to be the same size, as well as the same mode, and then computes the sum of the squared differences as an image similarity metric.\nYou can use the above fixture in a test via:\n1 2 3 4 def test_example(image_similarity): # test logic goes here and should generate an image in the # path given by image_similarity['filename'] pass When you add a new test, you need to add the expected image to tests/baseline_images/\u003ctestname\u003e.png.\n","description":"","tags":["tests","image","pytest","python"],"title":"A pytest fixture for image similarity","uri":"/post/pytest-image/"},{"categories":["Post","Testing","Software Development"],"content":"I’ve noticed a common area of misunderstanding for people newer to Python testing is how to apply mock.patch and where to patch (i.e. what the target positional argument to unittest.mock.patch should be), so I made a video explaining:\n","description":"","tags":["tests","python","101"],"title":"How to apply unittest.mock.patch","uri":"/post/unittest-mock-patch/"},{"categories":["Post","Cryptography"],"content":"The CBC padding oracle attack demonstrates how what might initially seem like a small issue can balloon into a devastating attack that can result in total reconstruction of the plaintext by the attacker. It’s also one of the harder challenges in Set 3 of Cryptopals.\nThe problem It goes like this: an attacker has access to an oracle that will take a ciphertext (i.e. what we need to decrypt) and return a boolean indicating whether or not the padding was valid. My padding oracle function looked like this:\n1 2 3 4 5 6 def cbc_padding_oracle(key: bytes, ciphertext: bytes, iv: bytes) -\u003e bool: try: aes_cbc_decrypt(key, ciphertext, iv, remove_padding=True) return True except BadPaddingValidation: return False where BadPaddingValidation was a custom exception indicating that - you guessed it - the padding was invalid.\nThe following (16-byte) block has valid padding:\nand our oracle will tell us that. This means that we learn something about the plaintext.\nFrom this fact alone, we can decrypt the ciphertext.\nHow it works Looking at how CBC decryption works, we can figure out how to use this fact to get the plaintext:\nWell, we don’t know the IV so let’s ignore that. We do have all the ciphertext blocks. And we can learn if the final N bytes of any given plaintext are valid via our oracle.\nLooking at the diagram we can see that:\n$c_{n-1} \\oplus \\mbox{decrypt}(c_n, k) = p_n$\nAs the attacker, we’ll copy $c_{n-1}$ to a test block we’ll call $t$ and introduce a single bit change in the final byte:\n$t \\oplus \\mbox{decrypt}(c_n, k) = p_n$\nWe’ll keep introducing single bit changes in the final byte until we get a valid response from the oracle. Then we’ve learned:\n$t[15] \\oplus \\mbox{decrypt}(c_n, k)[15] = 01$\nRearranging:\n$\\mbox{decrypt}(c_n, k)[15] = 01 \\oplus t[15]$\nMeaning that we learned about the final byte of the block cipher decryption output which we can reuse now with $c_{n-1}$ and $c_n$ to get the real final byte of plaintext:\n$p_n[15] = c_{n-1}[15] \\oplus \\mbox{decrypt}(c_n, k)[15]$\nThat’s the first byte reconstructed.\nLet’s go Starting at the rightmost block, we can move right to left decrypting each plaintext byte.\nFor bytes that aren’t the final byte in the block, we can use what we learned so far in the block to compute what the valid padding bytes would be for the bytes right of the target byte. For example, for the second-to-last byte, we are looking for padding that validates to:\nWe can compute the final byte of our test ciphertext such that it will equal our desired padding byte value 02:\n$t[15] = \\mbox{decrypt}(c_n, k)[15] \\oplus 02$\nSo as we decrypt each block we just need to keep track of the output of the block cipher prior to the xor so we can compute this.\nOne gotcha I got the above working pretty quickly, but realized my attack was not working reliably. Occasionally, I guessed the wrong answer early on in the plaintext reconstruction. Debugging I discovered the following case when reconstructing the first byte:\nThere can be multiple “correct” answers for a given byte, i.e. there can be multiple ciphertexts that will have valid padding. Here in the example we have two valid ciphertexts when computing the last byte in the ciphertext: one with 01 in the final byte position (what we’re looking for) and one where the second-to-last byte is 02 and produces a valid padding result (not what we’re looking for).\nTo handle this case, before settling on a given byte, I’d first modify the byte before the target byte and check if the padding was still valid:\n1 2 3 4 5 6 7 8 byte_num_to_edit = self.block_size + byte_num - 2 degeneracy_ciphertext = flip_nth_bit( full_test_ciphertext, byte_num_to_edit - self.block_size ) if cbc_padding_oracle(self.key, degeneracy_ciphertext, self.iv): pass else: continue This enabled me to distinguish between the two above cases, and ensure that there was only a single, correct answer for each byte.\nMy solution is here.\n","description":"","tags":["cryptography","cryptopals","block ciphers"],"title":"Implementing the CBC padding oracle attack","uri":"/post/cbc-padding/"},{"categories":["Post","Software Development"],"content":"Recently I wanted to generate UML (Unified Modeling Language) diagrams of the structure of an existing codebase for the purpose of having an architecture discussion.\nI was wondering if there was a tool to generate UML diagrams in Python to save me some manual work.\nEnter pyreverse: it comes installed with pylint which is a very common development dependency in Python. pyreverse enables you to point to the code you want UML diagrams of, here in my example I was generating a diagram of a project called securedrop-export:\n1 pyreverse securedrop_export/ This produces in the same directory a graphviz file called classes.dot.\nThen, provided you have graphviz (which provides the dot command) installed:\n1 dot -Tpng classes.dot -o securedrop_export.png Which produces the following:\nHere ExportStatus is an Enum and TimeoutException is a custom exception (in red).\n","description":"","tags":["UML","object oriented","python"],"title":"Using pyreverse to generate UML class diagrams","uri":"/post/pyreverse-uml/"},{"categories":["Post","Testing","Software Development"],"content":"I recently decided to try my hand at making YouTube videos. I’m planning on mostly making videos about topics I commonly find myself explaining to people, so here’s the first, on using pdb, the built-in debugger in the Python standard library.\n","description":"","tags":["tests","python","debugging"],"title":"Debugging programs with pdb","uri":"/post/pdb-debugging/"},{"categories":["Post","Testing","Software Development"],"content":"Test flakes are tests that occasionally fail due to a variety of potential reasons including network instability (for tests making network calls that are not mocked) and other non-deterministic behavior. Test flakes are problematic as they reduce confidence in the results of test runs: they condition developers that the test suite cannot be relied on, and as such can result in legitimate bugs being ignored due to alert fatigue.\nThis post contains some strategies for identifying and handling flaky tests in Python.\nIdentify, track, and fix One strategy is once a flake is identified to treat them like any other bug:\nFile an issue to track it such that there is awareness of the flakey test. Reproduce the flake locally. Fix whatever underlying reason is causing the flaky behavior. A common problem is to skip step 2, and instead take a guess why the flake is happening, and then modify the behavior. To prevent unnecessary implementation/review time, it’s worth resisting this urge and to at least try to understand the behavior prior to making changes. If it proves particularly troublesome to identify, then it may be reasonable to make some experimental fixes.\nOne strategy to identify the underlying problem is to use a plugin like pytest-repeat (pip install pytest-repeat) to run a single test a large number of times:\n1 pytest --count 100 tests/test_app.py::test_my_flaky_behavior You can do this on your main branch to reproduce the flake (adjusting the count as needed based on the failure rate of the test), and then run again once you have applied a fix to verify that it indeed resolves the issue.\nAutomatically re-run flaky tests Alternatively, a quick fix if you don’t want to take the time to address the underlying issues in the test suite is a pytest plugin called flaky (pip install flaky). One can use this to re-run individual flaky tests or classes of flaky tests up to a configurable number of times, e.g.:\n1 2 3 4 5 @flaky(max_runs=10) def test_my_flaky_behavior(): expected_result = 'result' actual_result = my_function('arg1', 'arg2') assert expected_result == actual_result Flaky test dashboard If you use a CI provider like Circle CI, you can see which tests are the most flaky (and use that to prioritize fixes) if you export test metadata:\n1 pytest --junitxml=~/project/test-results/junit.xml And then export them to Circle CI using their store_test_results and store_artifacts steps:\n1 2 3 4 5 - store_test_results: path: ~/project/test-results - store_artifacts: path: ~/project/test-results These tests results, as CI jobs fail due to flakes, will populate a dashboard at https://circleci.com/build-insights which will contain the most often failed tests, along with other useful information like the longest running failed tests.\n","description":"","tags":["tests","flakes","pytest","circleci"],"title":"Strategies for handling flaky test suites","uri":"/post/test-flakes/"},{"categories":["Post","Cryptography","Statistics"],"content":"How many people do you need in a room before there is a 50% chance that least two of them share the same birthday? It’s only 23, though unless you have heard about this paradox before, you might expect it to be much larger. This is the well-known birthday paradox: it’s called a paradox only because collisions happen much faster than one naively expects. Collisions here means an event where two or more observed values are equal. This paradox is important in cryptography as it’s relevant for many topics like cryptographic hash functions, which are designed to be collision-resistant one-way functions.\nThe probability of collision Let’s see why this is true by first deriving analytically the probability that no collision occurs for a generic case where there are $m$ possible values, and we observe $n$ of them. The birthday paradox above is a special case where $m=365$ and $n=23$. We want to know: what is the probability that we don’t see any collisions after $n$ observations?\nWe first note that the first observation, $o_1$, trivially occurs with probability 1: $P(o_1) = 1$.\nNext we compute the probability that the second observation does not match the first, there are now $m-1$ possible options out of $m$:\n$P(o_2 \\ne o_1) = \\frac{m - 1}{m} = 1 - \\frac{1}{m}$\nNow the third, which should not match either the first or the second:\n$P(o_3 \\ne o_1 \\ne o_2) = \\frac{m - 2}{m} = 1 - \\frac{2}{m}$\nAnd so on and so forth until the $n$th observation, which should not match any of the first $n-1$ observations:\n$P(o_n \\ne o_1 \\ne o_2 … \\ne o_{n-1}) = \\frac{m - (n - 1)}{m} = 1 - \\frac{n-1}{m}$\nThe probability of no collisions is just the joint probability of these events (assuming that they’re independent):\n$ P(\\mbox{No collision}) = \\prod_{i=1}^{n} P(o_i) = \\prod_{i=1}^{n-1} 1 - \\frac{i}{m}$\nWhat is the probability that a collision does occur? Well, we know that the probability that a collision occurs is the complement of the probability that no collision occurs, i.e.:\n$ P(\\mbox{Collision}) = 1 - P(\\mbox{No collision}) = 1 - \\prod_{i=1}^{n-1} 1 - \\frac{i}{m}$\nThat is the probability for any $n$ and $m$ that a collision occurs due to the birthday paradox ✨\nCollision probability as a function of $n$ Armed with the above analytic expression, we can write a simple function that computes the probability of collision for any $n$ and $m$:\n1 2 3 4 5 def prob_of_collision(n: int=23, m: int=365) -\u003e int: probability_of_no_collision = 1 for i in range(1, n - 1): probability_of_no_collision *= 1 - i/m return 1 - probability_of_no_collision Let’s now use this function to make a few plots showing how the probability changes as a function of $n$.\nBirthday case: m=365 We can see that the probability reaches $50%$ right around $n=23$, thus recovering what we stated in the introduction.\nLarger case: m=100000 For larger numbers, we see that collisions happen very fast: in a space of 100000 possible values, we reach $50%$ probability after less than 400 observations. This should underscore the importance of considering birthday attacks especially in the case even where the space of possible values is very large. We can see empirically from these couple of cases that the number of observations $n$ you need to get a probability of collision of 0.5 is approximately $\\sqrt{m}$. You can prove that to yourself analytically by taking the equation we derived above for the probability of collision, setting the left hand side equal to 0.5, and rearranging to see the relationship between $n$ and $m$.\n","description":"","tags":["cryptography","101","teaching","statistics","hash functions"],"title":"Collision attacks and the birthday paradox","uri":"/post/birthday-attacks/"},{"categories":["Post","Transparency","Censorship"],"content":"This was written with Camille Fassett and first appeared on the website of the Lucy Parsons Labs.\nMotivated by the Trump administration’s policy of detaining and separating migrant children from their families at the border, artist and developer Sam Lavigne harvested profiles that list Immigration and Customs Enforcement (ICE) as an employer on social network LinkedIn.\nHe made the dataset of all 1595 responsive profiles, which included names, titles, photographs and general locations, available to download on GitHub. The repository also included the Python script that he used to scrape LinkedIn, along with basic instructions on how to replicate his work.\n“While I don’t have a precise idea of what should be done with this data set, I leave it here with the hope that researchers, journalists and activists will find it useful,” Lavigne wrote in a Medium post about the profiles he scraped.\nBut at the URL where the dataset was hosted is now just a page that reads: “This repository has been disabled. Access to this repository has been disabled by GitHub staff.” And in place of Lavigne’s explainer on Medium is simply, “This page is unavailable.”\nGitHub and Medium aren’t the only entities that scrubbed Lavigne’s work from their platforms. Twitter also suspended @iceHRgov, an account that tweeted profiles from the dataset.\nA spokesperson for GitHub stated that the repository was removed because it violated its community guidelines—specifically, those that prohibit doxxing, harassment, and violating a third party’s privacy.\nBut doxxing is targeting private individuals for harassment by exposing their private, personal information. ICE is a government agency, and its employees on the ground are in public, acting in their capacity as government officials. As journalist Sarah Jeong, the author of a book about online harassment titled The Internet of Garbage, has written, “The definition of doxing is the publication of a physical residential address, or information protected by law (social security numbers, medical records, and so forth).”\n“Unmasking someone by their full name, identifying someone by their first name, identifying their place of work, or screencapping e-mails are not doxing. They are—once again, depending on the circumstances—possibly abusive things to do. But they are not doxing,” Jeong continues. “Do you know what is an abusive thing to do? To expand the definition of doxing in order to harness public outrage without having to actually discuss the circumstances in which you have been exposed.”\nLavigne only collected information about ICE employees that was related to their work - information that they themselves chose to share publicly. Anyone with a LinkedIn profile could view the same information. While doxxing can be frightening, Lavigne did not share information such as their home addresses, telephone numbers, or other details about their private lives or families.\nThe idea that law enforcement officers should be identifiable is not new or radical. This basic transparency measure is why officers have badge numbers and carry identification cards that include their names. This principle was reiterated by the police chief of Hartford, CT (a sanctuary city), who said: “All law enforcement officials, not acting in an undercover capacity, working in our community should be readily identified by the agencies that they represent.”\nICE agents are no different. When carrying out their public duties, they should be easily identified by the public as both ICE employees and individuals. Failing to identify themselves is cause for public concern.\nThere are countless instances of very real online harassment and threats—frequently targeted at women, immigrants, queer and trans people, and people of color. In many of these cases, platforms like Twitter have often been far slower to act to remove content that targets vulnerable people than it does to protect ICE officials participating in a regime separating children from families and deporting hundreds of thousands of immigrants per year.\nSharing information about government agents related to the execution of their public duties for the purposes of government accountability and transparency should never be considered doxxing or harassment.\n","description":"","tags":["transparency","censorship","public interest","accountability"],"title":"Why Publishing Data on ICE Employees is in the Public Interest","uri":"/post/publishing-public-interest/"},{"categories":["Post","Cosmology","JavaScript","d3.js"],"content":"Cosmologists make observations of galaxies and radiation in the universe to constrain parameters of the Lambda CDM model , which is the model that best describes our current understanding of the universe. These cosmological parameters include quantities like Ωm and ΩΛ, which describe the matter and dark energy content of the universe respectively.\nTwo key observables that constrain these parameters are the matter power spectrum and the angular power spectrum of the Cosmic Microwave Background (CMB) radiation. Let’s briefly go over what each of these observables describes.\nMatter Power Spectrum The matter power spectrum - P(k) - describes the large scale structure of the universe - it tells us on which scales matter is distributed. P(k) is a function of wavenumber k which k corresponds to inverse scale - so increasing wavenumber means decreasing scale.\nFor example, using the visualization, we can see that by increasing the matter content of the universe, we increase the power at large wavenumber, which corresponds to smaller scales. This occurs due to enhanced structure formation as a result of the additional matter content.\nCMB Angular Power Spectrum The Cosmic Microwave Background (CMB) radiation was produced only ~100,000 years after the Big Bang, and gives us information about the universe at these early times. One observable from this radiation is the CMB angular power spectrum, which describes anisotropies in the temperature of the CMB radiation as a function of angular scale. Since this temperature anisotropy is defined over a sphere, the temperature anisotropy is separated out into angular scales using a multipole expansion (to read how this multipole expansion is done in detail, check out these notes (PDF)). The spectrum Cℓ (plotted as ℓ(ℓ+1)Cℓ) is a function of multipole ℓ, so increasing multipole corresponds to decreasing angular scales.\nVisualizing Power Spectra Note: This visualization was created in collaboration with Eric Baxter, astrophysics postdoc at UPenn.\nTo visualize how these power spectra change as a function of cosmological parameters, we used d3.js, a popular JavaScript library for creating interactive visualizations.\nLaunch the visualization here! As the user moves a given slider, the spectra are updated by linearly interpolating between the nearest pre-computed spectra. To create the pre-computed spectra, we used CAMB, a software package for computing power spectra based on input cosmological parameters to generate power and CMB spectra around a fiducial value. We selected a fiducial model from the 2015 results from the Planck Collaboration (PDF). The universe was kept spatially flat (i.e. Ωm + ΩΛ = 1) both because it is a well established constraint and to reduce the amount of data that we needed to generate with CAMB.\nIf you’d like to check out the code, it’s available here on GitHub. If you’d like to learn more about the effect of cosmological parameters on the power spectra, check out this tutorial.\n","description":"","tags":["astrophysics","cosmology","d3.js","visualization"],"title":"Visualizing Cosmological Power Spectra with d3.js","uri":"/post/visualizing-cosmological-power-spectra/"},{"categories":["SecureDrop","whistleblowing"],"content":"When people first learn of the SecureDrop architecture, it can seem quite complicated. When explaining SecureDrop to people for the first time, it’s useful to have a cartoon view providing a broad overview before digging into further technical details (in my experience this is true of any sufficiently technical concept/system). After noticing this confusion from users, I made this quick one-minute video to distill the most important points for the source and journalist workflows:\n","description":"","tags":["SecureDrop","101","whistleblowing"],"title":"How SecureDrop Works 101","uri":"/post/securedrop-101/"},{"categories":["Chicago","Police","LPL"],"content":"Every year, police take millions of dollars from Chicagoans through civil asset forfeiture and spend it behind closed doors. In collaboration with Muckrock, my co-authors and I did an independent financial audit of this previously undocumented part of the police budget. We demonstrated these asset forfeiture funds are used to circumvent city council oversight and finance controversial surveillance devices. This investigation appeared as a front page feature in the September 29, 2016 issue of the Chicago Reader.\nRead the full investigation here.\n","description":"","tags":["police","asset forfeiture","FOIA","data analysis"],"title":"Inside the Chicago Police Department’s Secret Budget","uri":"/post/cpd-asset-forfeiture/"},{"categories":["Post","Police","Surveillance"],"content":"I wrote a short primer describing and visualizing police surveillance technology. This project was also posted on the website of the Lucy Parsons Labs.\nCameras, automatic license plate readers, cell site simulators and many other surveillance devices are currently used in the city by the Chicago Police Department and its sister agencies. However, many citizens are unaware of the scope of the surveillance systems, their huge cost, and the privacy implications of their use. Lucy Parsons Labs has written a primer on police surveillance devices in the city, and will be updating it as we learn more through our audit in collaboration with Muckrock and our other investigations.\nChicago residents have legitimate questions surrounding the use of these techologies: Are appropriate policies and procedures in place to ensure these systems are responsibly used? To what degree are these systems necessary or even useful? Do the benefits of these systems justify the tens of millions of dollars of taxpayer dollars being spent on them? Do members of the public want this level of surveillance to be used?\nIn the past, police surveillance of public spaces was constrained by resources such as availability of police officers. However, surveillance technology is producing a significant expansion of police capabilities. Police were historically unable to follow every resident with a police car, but with new surveillance technologies they may be able to effectively accomplish the same goal albeit in a less disruptive manner.\nMembers of the public need to recognize the far-reaching implications of giving over this level of power to law enforcement. The transition from police officers having a restricted ability to follow and monitor suspicious people to having pervasive surveillance capabilities is occurring in our city today. We must have public knowledge of surveillance capabilities, policies and procedures in order to have the critical debate about this infrastructure as well as to campaign for transparency and accountability.\nCheck out the full primer here.\n","description":"","tags":["police","surveillance"],"title":"Primer on Police Surveillance","uri":"/post/police-surveillance-primer/"},{"categories":["machine learning","ethics"],"content":"This is a writeup of a talk that I gave at Defcon 23’s Cryptography and Privacy Village back in August 2015. The presentation included a brief primer on machine learning as some in the audience were unfamiliar with the topic, so skip down to “Is there really a problem here?” if you are already familiar with ML. Thanks to all who attended and for the excellent questions and comments! References are embedded as links in the text.\nIntroduction As a society we have an ever-increasing amount of information, data which only has value if we are able to extract meaningful insights - the needle in the haystack. In data science, we use machine learning algorithms and methods like A/B testing to do this. Data science enables us to learn about systems, determine how to optimally make decisions and infer new information not present in the data. These data analysis techniques are incredibly useful and are becoming more powerful due to more and better data as well as the due to the development of increasingly more intelligent systems. These methods have spread into a wide range of areas:\nLet’s step through a few of the problems that machine learning can be used for:\nAdvertising: This is probably the most familiar application. The main difference here is that we’re increasingly moving from very coarse-grained advertising systems to extremely finely targeted ads. Political campaigns: Machine learning is used to build predictive models about individual voters to determine which voters a campaign should target and build experiments that can be used to determine what the optimal action the campaign can take to bring about their desired goal. Surveillance: Machine learning can be used to classify people into suspicious or not, such as the system that puts people onto the no-fly list. By machine learning here, we’re referring to a set of computer programming techniques that adaptively learn from data: they learn programs from data instead of requiring a set of rules to be explicitly written down by a programmer.\nFor example, if I want to write a program that recognizes handwritten characters, I can write down a list of rules, telling the computer specifically what shapes to expect in a given character. I would need to ensure that the program also can handle different handwriting styles as well as both printed and cursive text. Even for a single character this process could get quite laborious considering the wide variation in handwriting. Instead, one could use supervised machine learning, where a computer learns the rules that best identify a character using many examples of handwritten letters.\nSupervised learning methods are what we will be focusing on in this discussion. First, we’ll look at a cartoon example to set up some nomenclature and build some intuition about how this is done.\nLet’s take a canonical classification problem: I want to classify some images into cats, which I will denote with orange circles, and dogs, denoted by blue circles. In order to use supervised learning to approach this problem, I must get examples of past images and whether they were classified as a cat or a dog - this is my training set. I then take both my training and testing data and construct features, quantities in the data that might be predictive of the target - whether a given image is a cat or a dog. These features could be quantities as simple as the intensity in a given pixel or the mean color in an image. Here’s a cartoon view of these training examples distributed in feature space:\nThen we take these examples and their derived features along with a learner - a machine learning algorithm - which will dynamically determine which features are predictive of the label and by how much.\nThen, this trained model can be used with a new example from our testing data to predict for the new example whether or not the image contains a cat or a dog, based on what side of the decision boundary the example falls on.\nThis is a very brief and high level overview of the nomenclature that we’ll be using in the remainder of the discussion.\nIs there really a problem here? From the applications I describe above, it’s clear that many of these machine learning systems can and will have profound positive impacts on human society. For example, autonomous vehicles have a huge potential for reducing human injury and death due to car accidents, one of the most common causes of death in the United States. Predictive systems can be used to make better use of taxpayer dollars, as well as can help find and correct problems like the use of lead paint in homes before serious damage or injury occurs.\nBut these systems are also used for applications that are not as noble, as in the case of mass surveillance. And there might be unintended consequences of relying on systems like these, which the public and sometimes even the designers may not fully understand. We need to make sure that we are thinking about the ramifications of our reliance on machine learning methods with respect to the principles that the privacy advocacy community cares about. In the privacy advocacy community we discuss surveillance a great deal for good reason. So to broaden the scope of the discussion, in this talk I’m going to focus on potential problems due to algorithms beyond surveillance and discuss other issues related to implementation and power. It would be great to have more discussion in both the privacy advocacy and data science communities about this topic.\nIn terms of problems that can arise, I’ll discuss implementation issues related to biased input data as well as usage issues, when systems are deployed that can be easily misused due to little or no oversight.\nInput Datasets As explained in the introduction, these are systems that learn by example. The examples must be representative of the truth. If the input data are not collected in an unbiased way, then the model will be biased in some way. In an ideal world, training datasets would be colleced by random sampling, where the probability of collecting an example is uniform, however, most sampling out there in the real world is not random. Strong selection effects are present in most training data, and in order to generate an accurate model, these need to be understood. Problems arise when input data is treated as unbiased and is used as input to an algorithm which is then treated as impartial and unbiased. Let’s go back to our cartoon view of examples in feature space to see why this occurs:\nHere we have our examples in feature space, with a decision boundary drawn through the examples. But let’s zoom out:\nWe see that the decision boundary extends into regions of feature space that our examples here are not providing much information about. Outside this dotted region, the model is basically unconstrained.\nEven with a few examples, the coverage is so sparse that a few examples don’t help the model much.\nWhen the input data is biased in this way, we end up with a highly biased model.\nThe model incorrectly classifies many examples. This is one major issue that is often handled poorly. Let’s look at a real world example to see how this might manifest itself in algorithms that are currently in use.\nPredictive Policing Police departments state that they are starting to direct more energy to crime prevention, which is one way that machine learning is used by police departments. The idea is to allocate resources more effectively and enable proactive and preventative policing strategies. Perhaps by allocating police officers to areas where crimes are likely to occur then you can prevent those crimes from even occurring: the mere presence of police will dissuade people from criminal acts. In the same vein, police officers could also surveil those individuals who are considered most likely to commit crimes. Here’s how the New York City police commissioner regards predictive policing:\nThese two types of crime prediction systems are:\nIndividual: Crime probabilities for individual people. Used by the Chicago Police Department, which maintains a “Heat List” of people who are considered most likely to commit some violent crime. This is done using network analysis going two hops out. Spatially: An approach that is considered more tasteful (in terms of not evoking Minority Report as much) is to predict which areas are likely to have crimes committed in them within some time period. One issue immediately is that since humans tend to cluster and the same individuals tend to aggregate in the same areas, these two approaches are not totally dissimilar. One of the major players in this space is a company called PredPol. They have a product that predicts crime probabilities on a spatial basis. On their website they say the following:\nTheir system does not use individual data: they use only type, place and time of crime. However, statements like this in isolation should in no way reassure people of the unbiased nature of predictive crime systems like these. There are inhereant biases in input data due to selection effects in input data. Selection effects include biases due to people not being equally likely to call the police. In addition, we know that for many crimes, such as marijuana-related offenses, white and black people infract at a similar rate, black people are far more likely to be charged with marijuana-related crimes due to biases in law enforcement.\nIf I were to naively feed this crime data into the model, I’ve unevenly sampled feature space, and could very well be oversampled in areas of low socioeconomic status. The model will then not be representative of the truth and policing strategies based on a model will send cops to these poor areas, thus systematizing the discrimination. The real danger here with systems like this is that they could easily become a self-fulfilling prophesy if improperly validated.\nThis can be done in a more responsible way. One way to proceed in a scenario like this is to allocate some fraction of your resources to targeting regions of feature space that are poorly constrained by the model. To make this system more fair, you don’t just need more data but the right data.\nThere are lots of other ways of dealing with biased training data such as reweighting the training data in feature space. There’s also a body of work in the machine learning literature on fair representations that aim to address some of the issues related to discrimination.\nHowever, naive use of training sets is just one issue. Spatial aggregation doesn’t wash all these problems away, especially when the aggregation is done over very small areas. Predpol for example aggregates over 500 ft x 500 ft blocks, so this could be only a few houses. In this way aggregated data can still provide identifying information about individuals. Clearly, removing controversial features like race, criminal history, or even generally individual-level information does not magically erase all discriminatory issues with the training data.\nI’m certainly not necessarily condemning all predictive systems here. But these are the sorts of questions we need to be asking manufacturers of these systems to ensure that they have thought carefully about these issues. Currently, there is no formal system to regulate or address these types of discrimination and bias concerns when it comes to machine learning implementation.\nFiltering Even if one is able to construct a system that is perfectly accurate, another concern is how these methods are used. One possibility that has been raised is the potential for manipulation of algorithms by the organizations that control them, their employees, or by attackers. These manipulations would be difficult to detect due to a current lack of transparancy and accountability.\nA set of algorithms that are natural targets for manipulation are filtering algorithms. Filtering algorithms are necessary due to the firehose of data we are all currently trying to make sense of in our daily lives. We can sort through this data in a variety of ways:\nReverse chronological order: Look at the newest items first Collaborative filtering: People vote on what they think is important Algorithmic filtering: Algorithms decide what should be shown or ranked highly One such algorithm for algorithmic filtering is the Facebook news feed. This system takes a list of potential news feed items, builds features from them, puts them into a modeling system, and then produces an output of some ranked list of news feed items.\nWe have some idea of the features that are important for this system:\nIs a trending topic mentioned? Is this an important life event? Marriage, congratulations, etc. How old is the news item? How many likes or comments does this item have? How many likes or comments by people that I know? Is offensive (as judged by Facebook) content present? The operation of this algorithm is important to understand as it determines what updates and news stories a Facebook user gets to see. In a scenario where the platform has over a billion users and a huge and increasing segment of the population relies on Facebook to get their news, we will see that even a small change can have a huge impact.\nManipulation One high impact study discussed in the media regarding the manipulation of the Facebook news feed algorithm was Experimental evidence of massive-scale emotional contagion through social networks. In this work, Facebook took almost 700k users and segmented them into control groups and positive and negative groups. The positive group was shown more items in their news feed that consisted of positive words. The negative group was shown more items in their news feed that consisted of negative words. Then Facebook observes the news items that are posted in response. And they find that in the positive group, more positive news items are posted, and in the negative group, more negative news items are posted. This is a statistically significant effect, because very tiny differences in the statistical properties of news feed items shown to users can have huge impacts.\nSince you might be wondering, this type of work is legal if you accepted Facebook’s Data Use Policy. Agreeing to this document constitutes informed consent for this type of experimentation. Facebook is able to use this type of mechanism to shift people’s emotional states. However, there was another study that demonstrated using a similar mechanism, specifically by showing people a message about it being election day and showing them that some of their friends had voted, Facebook can statistically significantly increase the number of people that are likely to go out and vote. This is a lever that can be used to condition entire populations to act in a certain desired direction without them being aware. That’s a very scary idea and is the ultimate in mass social engineering.\nIn addition to corporations, their employees, and anyone that may infiltrate these organizations, governments are a natural entity to be interested in such powerful mechanisms of control. I encourage you to take a look at documents like this one from the UK’s GCHQ examining how to use psychological techniques to condition individuals towards obedience and conformity.\nThe issues associated with the usage of these algorithms is their opaque operation on proprietary data. These systems can enable manipulation. For all the issues described with algorithms, we need more information. The lack of information about their operation is the fundamental issue here.\nThe real problem is that we have no idea how many of these issues we are detecting. Many of the controversies that have occurred in recent years related to algorithms have been due to papers or admissions by the companies themselves. Both of the studies on manipulating the Facebook news feed were published by Facebook researchers. We need to make sure that we are building tools and advocating for policies that will enable the early detection of manipulation and discrimination and ensure that there is a mechanism for redress.\nTime for action The first step is stronger consumer protections. People should know how their data is being used and to that end we need more explicit data use and privacy policies. Users need this so they can make an informed decision. Facebook and other companies should more clearly and explicitly state how they use their users’ data, especially with respect to experimentation. There should also be a capacity for users to opt-out (or even better: opt-into) experimentation. There are bodies like the Federal Trade Commission in the US that can investigate and pursue companies that do not follow their own policies.\nNext, all these systems leverage data. The situation is that the party who has access to the data as well as has the skills and resources to leverage the data has the power. Obviously reducing the amount of data collected helps - something that can be done by building systems that are private by design. For example, there’s a lot of work that has been done in the academic community on schemes for privacy-preserving targeting advertising.\nAnother front to push forward on is advocating independent audits, open algorithms, and transparency. A nice feature from some recommendation systems like amazon is the ability to enable you to determine why a given result was ranked highly, e.g. showing what other items have you purchased. Providing rankings of feature importances is also incredibly useful for transparency reports. Of course, companies have no reason to do these things unless consumers or regulation demands it.\nIn terms of technology, one approach is to examine the algorithms themselves with black box analysis: correlating inputs and outputs to study the system in the black box. There is a schematic view of the facebook news feed algorithm. One can generate inputs using test accounts or real accounts. The outputs can be compared and differences in outputs across users can be examined.\nOne great recent example of how this analysis can be used to increase transparency is Xray. This is a really nice project done in the context of determining how ad placement is performed. The idea is to use test accounts, provide some interesting keywords as input and then see what ads get served in response. This can demonstrate that sensitive topics can be targeted and can also demonstrate data abuse. The group found some nice results when they applied this tool to Gmail: for example, when keywords associated with being in debt were present, they were served ads for new cars. If Google wasn’t reading your email then this wouldn’t be possible. But since they are reading your email, you can apply this type of analysis and find out how they are using your data. Similar techniques could be applied to other platforms. This is an excellent way forward that does not necessarily require the consent of the platforms involved.\nMoving Forward To practitioners: We need to be careful that we are using these systems in an equitable and responsible manner. Given biased input data, algorithms are not impartial and should not be treated as such. If we are to design systems that treat people fairly, then we need to be sure we carefully construct these systems not to discriminate.\nAnd for us advocates, we need to recognize that these methods are extremely powerful and we have to demand more information and more accountability in order to monitor how they are designed and being used. We should push forward with both policy and technology to achieve this. The privacy advocacy community has a big role to play to show people what is being done here, because by and large we don’t know. And we’re not going to know unless we find out.\n","description":"","tags":["machine learning","bias","fairness","ethics"],"title":"Manipulation and Machine Learning","uri":"/post/manipulation-ml/"}]
